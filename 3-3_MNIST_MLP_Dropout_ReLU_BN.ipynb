{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Module Import\n",
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import torch # deep learning framework pytorch\n",
    "import torch.nn as nn # module including functions needed for deep learning/ai\n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using PyTorch version: 1.7.1 Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Identify device used for Deep Learning module\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = torch.device('cuda')\n",
    "else:\n",
    "  DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, 'Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 #모델 학습시 필요한 데이터 \n",
    "EPOCHS = 10 # batch 개 단위로 back propaagation을 이용해 MLP weight 업데이트 \n",
    "# hyperparameters usually capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST 데이터 다운로드 \n",
    "\n",
    "train_dataset = datasets.MNIST(root = \"data/MNIST\", #root: 데이터가 저장될 장소 지정\n",
    "                               train = True, #대상 데이터가 학습용\n",
    "                               download = True, #해당 데이터 인터넷에서 다운로드 \n",
    "                               transform = transforms.ToTensor()) #손글씨 '이미지'데이터 불러옴과 함께 전처리\n",
    "                               #tensor 형태로 데이터 불러옴, 0~255 범위의 픽셀 값 0~1 범위로 정규화\n",
    "test_dataset = datasets.MNIST(root = \"data/MNIST\", \n",
    "                              train = False, #대상 데이터가 검증용\n",
    "                              transform = transforms.ToTensor())\n",
    "#정규화해 불러온 train, test 데이터셋 mini-batch 단위로 분리해 지지정\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True) #데이터 순서 섞기\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type:  torch.FloatTensor\nY_train: torch.Size([32]) type:  torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "## 데이터 확인하기 -> 다운로드 한 후 mini-batch 단위로 할당한 데이터의 개수, 형태 확인\n",
    "\n",
    "for (X_train, y_train)in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type: ', X_train.type())\n",
    "    print('Y_train:', y_train.size(), 'type: ', y_train.type())\n",
    "    break\n",
    "\n",
    "# X_train: torch.Size([32, 1, 28, 28]) type:  torch.FloatTensor \n",
    "# --> 32개의 데이터로 batch 구성, 채널 1(그레이스케일), 28x28픽셀의 이미지\n",
    "# Y_train: torch.Size([32]) type:  torch.LongTensor\n",
    "# --> 32개의 이미지 데이터 각각에 label값 1개씩 존재, 32개의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP(Multi Layer Perceptron) 모델 설계\n",
    "\n",
    "class Net(nn.Module):   #PyTorch Moudle 내 딥러닝 모델 관련 기본 함수 포함하는 nn.Module 클래스 상속받는 Net 클래스 정의\n",
    "    def __init__(self): #Net 클래스의 instance 생성시 지니게 되는 성질 정의\n",
    "        super(Net, self).__init__() #nn.Module 내의 메서드 상속받아 이용 \n",
    "        self.fc1 = nn.Linear(28*28, 512)    #첫 번째 Fully Connected Layer 정의, input size: 28*28*1, output node 수: 512\n",
    "        self.fc2 = nn.Linear(512, 256)      #두 번째 Fully Connected Layer 정의, input: 512, output: 256 \n",
    "        self.fc3 = nn.Linear(256, 10)       #세 번째 Fully Connected Layer 정의, input: 256, output: 10 (0~9까지 10개의 클래스 표현 위한 라벨 one-hot-encoding)\n",
    "        self.dropout_prob = 0.5 #50%의 노드에 대해 가중값 계산하지 않음 \n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)  #nn.BatchNorm1d() -> 1차원 벡터 batch normalization \n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    # Batch Normalization 적용하는 부분 -> 논문에 따라 activation function 이전/이후 적용 선호도 갈림\n",
    "\n",
    "    def forward(self, x):   #clss Net 이용해 설계한 MLP 모델의 Forward Propagation정의 -> output 계산하기까지 과정 나열\n",
    "        x = x.view(-1, 28*28)   #2차원 데이터 1차원 데이터로 변형, view 메서드 사용 - 784크기의 1차원 데이터로 변환(flatten)\n",
    "        x = self.fc1(x)     #첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터 통과\n",
    "\n",
    "        x = self.batch_norm1(x) #activation func 이전에 batch normalization 진행\n",
    "\n",
    "        x = F.relu(x)    #torch.nn.functional 에 정의된 비선형 함수 relu() 이용해 fc2의 input 계산\n",
    "\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)   #relu()함수의 결과값에 dropout 적용\n",
    "\n",
    "        x = self.fc2(x)     #두 번째 Fully Connected Layer에서 relu() 함수로 계산된 결과값 통과\n",
    "\n",
    "        x = self.batch_norm2(x) #두 번째 Fully Connected Layer의 output batch normalization\n",
    "\n",
    "        x = F.relu(x)    #torch.nn.functional 에 정의된 relu() 이용해 fc3의 input 값 계산\n",
    "\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "\n",
    "        x = self.fc3(x)     #세 번째 Fully Connected Layer에서 relu() 함수로 계산된 결과값 통과\n",
    "        x = F.log_softmax(x, dim = 1)   #torch.nn.functional 내의 log_softmax 이용해 최종 output 계산\n",
    "        #softmax 이용해 0~9까지 10개의 클래스에 속할 확률 값 계산 \n",
    "        #log_softmax -> 일반적인 softmax 보다 Back Propagation 이용해 학습시 loss 값에 대한 gradient값 보다 원활하게 계산(log 함수의 기울기 보다 부드럽게 변화)\n",
    "        \n",
    "        return x    #최종 계산된 x 값 output으로 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (fc1): Linear(in_features=784, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n"
     ]
    }
   ],
   "source": [
    "## Optimizer, Objective Function 설정하기\n",
    "\n",
    "model = Net().to(DEVICE)    #앞서 정의한 모델을 DEVICE에 할당\n",
    "#Back Propagation 이용해 파라미터 업데이트시 이용하는 optimizer 정의\n",
    "#Stochastic Gradient Descent(SGD) 알고리즘 이용, Learning Rate 0.01, Momentum(Optimizer의 관성 나타냄) 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)  \n",
    "\n",
    "#MLP 모델의 output 값 + one-hot-encoding값과의 loss Criterion 이용해 계산 \n",
    "criterion = nn.CrossEntropyLoss()   #nn.CrossEntropyLoss() 로 criterion 설정\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Net(\n",
    "#   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
    "#   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
    "#   (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP 모델 학습 진행, 학습 데이터에 대한 모델 성능 확인\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()   #MLP 모델을 학습 상대로 지정\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):    #train_loader: 이미지+레이블 데이터 batch로 묶여서 저장\n",
    "                                                                #train_loader의 batch별로 저장된 데이터 순서대로 이용해 MLP 모델 학습시킴\n",
    "        image = image.to(DEVICE)    #mini-batch의 이미지 데이터 활용하기 위해 device에 할당\n",
    "        label = label.to(DEVICE)    #mini-batch의 레이블 데이터도 device에 할당\n",
    "        optimizer.zero_grad()       #기존의 device에 할당할 경우 과거에 이용한 데이터를 바탕으로 계산된 loss와 gradient값이 optimizer에 할당되어 있으므로 optimizer의 gradient값을 초기화\n",
    "        output = model(image)       #장비에 할당한 이미지 데이터를 MLP 모델의 input으로 output계산\n",
    "        loss = criterion(output, label) #계산된 output을 label값과 함께 기존에 정의한 cross entropy를 이용해 loss 계산\n",
    "        loss.backward()     #loss계산값을 바탕으로 back propagation -> 계산된 gradient 값 각 parameter에 할당\n",
    "        optimizer.step()    #각 파라미터에 할당된 graidient값을 이용해 파라미터 값 업데이트\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\t Train Loss: {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image),\n",
    "                len(train_loader.dataset), 100. *batch_idx / len(train_loader),\n",
    "                loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습되는 과정 중 검증 데이터에 대한 모델 성능을 확인\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()    #학습 과정, 학습 완료된 MLP 모델을 학습 상태가 아닌, 평가 상태로 지정\n",
    "    test_loss = 0   #test_loader 데이터를 이용해 loss 값 계산 위해 test_loss 0으로 임시설정\n",
    "    correct = 0     #학습 완료된 MLP 모델이 올바른 Class로 분류한 경우 세기 위한 변수\n",
    "\n",
    "    with torch.no_grad():   #torch.no_grad(): 모델 평가하는 단계에서 gradient통해 파라미터 값이 업데이트 되는 현상을 방지 \n",
    "        for image, label in test_loader:    #test_loader batch 단위로 저장 -> for 문으로 차례대로 접근\n",
    "            image = image.to(DEVICE)    #test_loader의 mini-batch내의 데이터 device에 할당\n",
    "            label = label.to(DEVICE)    #test_loader의 label 데이터 device에 할당\n",
    "            output = model(image)       #MLP 모델 output계산\n",
    "            test_loss += criterion(output, label).item()    #output과 label 값에 대해 CrossEntropy를 이용해 loss 계산-> test_loss 업데이트 \n",
    "            prediction = output.max(1, keepdim = True)[1]   #MLP 모델의 output -> 크기 10인 벡터값\n",
    "                                                            #계산된 벡터 값 내 가장 큰 값의 위치에 해당하는 클래스 예측\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()    #예측값 == label값일 때 correct 변수 1 증가(올바르게 예측한 횟수)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)   #현재까지의 test_loss를 batch 개수만큼 나누어 평균 loss 값 구함\n",
    "    test_accuracy = 100. *correct / len(test_loader.dataset)    #test_loader 데이터 중 얼마나 맞추었는지 정확도 계산\n",
    "    return test_loss, test_accuracy  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\t Train Loss: 2.631836\n",
      "Train Epoch: 1 [6400/60000(11%)]\t Train Loss: 0.674329\n",
      "Train Epoch: 1 [12800/60000(21%)]\t Train Loss: 0.313450\n",
      "Train Epoch: 1 [19200/60000(32%)]\t Train Loss: 0.388461\n",
      "Train Epoch: 1 [25600/60000(43%)]\t Train Loss: 0.383792\n",
      "Train Epoch: 1 [32000/60000(53%)]\t Train Loss: 0.345885\n",
      "Train Epoch: 1 [38400/60000(64%)]\t Train Loss: 0.853530\n",
      "Train Epoch: 1 [44800/60000(75%)]\t Train Loss: 0.180082\n",
      "Train Epoch: 1 [51200/60000(85%)]\t Train Loss: 0.462678\n",
      "Train Epoch: 1 [57600/60000(96%)]\t Train Loss: 0.472164\n",
      "\n",
      "[Epoch: 1], \t Test Loss: 0.0050, \t Test Accuracy: 95.26 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\t Train Loss: 0.194151\n",
      "Train Epoch: 2 [6400/60000(11%)]\t Train Loss: 0.303233\n",
      "Train Epoch: 2 [12800/60000(21%)]\t Train Loss: 0.162486\n",
      "Train Epoch: 2 [19200/60000(32%)]\t Train Loss: 0.317404\n",
      "Train Epoch: 2 [25600/60000(43%)]\t Train Loss: 0.184256\n",
      "Train Epoch: 2 [32000/60000(53%)]\t Train Loss: 0.411597\n",
      "Train Epoch: 2 [38400/60000(64%)]\t Train Loss: 0.336363\n",
      "Train Epoch: 2 [44800/60000(75%)]\t Train Loss: 0.340106\n",
      "Train Epoch: 2 [51200/60000(85%)]\t Train Loss: 0.109361\n",
      "Train Epoch: 2 [57600/60000(96%)]\t Train Loss: 0.309690\n",
      "\n",
      "[Epoch: 2], \t Test Loss: 0.0037, \t Test Accuracy: 96.53 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\t Train Loss: 0.191554\n",
      "Train Epoch: 3 [6400/60000(11%)]\t Train Loss: 0.212107\n",
      "Train Epoch: 3 [12800/60000(21%)]\t Train Loss: 0.467000\n",
      "Train Epoch: 3 [19200/60000(32%)]\t Train Loss: 0.210306\n",
      "Train Epoch: 3 [25600/60000(43%)]\t Train Loss: 0.285291\n",
      "Train Epoch: 3 [32000/60000(53%)]\t Train Loss: 0.206798\n",
      "Train Epoch: 3 [38400/60000(64%)]\t Train Loss: 0.074891\n",
      "Train Epoch: 3 [44800/60000(75%)]\t Train Loss: 0.093346\n",
      "Train Epoch: 3 [51200/60000(85%)]\t Train Loss: 0.256962\n",
      "Train Epoch: 3 [57600/60000(96%)]\t Train Loss: 0.319957\n",
      "\n",
      "[Epoch: 3], \t Test Loss: 0.0031, \t Test Accuracy: 96.82 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\t Train Loss: 0.164259\n",
      "Train Epoch: 4 [6400/60000(11%)]\t Train Loss: 0.377440\n",
      "Train Epoch: 4 [12800/60000(21%)]\t Train Loss: 0.045134\n",
      "Train Epoch: 4 [19200/60000(32%)]\t Train Loss: 0.101710\n",
      "Train Epoch: 4 [25600/60000(43%)]\t Train Loss: 0.428483\n",
      "Train Epoch: 4 [32000/60000(53%)]\t Train Loss: 0.041974\n",
      "Train Epoch: 4 [38400/60000(64%)]\t Train Loss: 0.095259\n",
      "Train Epoch: 4 [44800/60000(75%)]\t Train Loss: 0.270667\n",
      "Train Epoch: 4 [51200/60000(85%)]\t Train Loss: 0.400620\n",
      "Train Epoch: 4 [57600/60000(96%)]\t Train Loss: 0.045009\n",
      "\n",
      "[Epoch: 4], \t Test Loss: 0.0027, \t Test Accuracy: 97.30 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\t Train Loss: 0.195937\n",
      "Train Epoch: 5 [6400/60000(11%)]\t Train Loss: 0.155735\n",
      "Train Epoch: 5 [12800/60000(21%)]\t Train Loss: 0.188694\n",
      "Train Epoch: 5 [19200/60000(32%)]\t Train Loss: 0.109856\n",
      "Train Epoch: 5 [25600/60000(43%)]\t Train Loss: 0.231155\n",
      "Train Epoch: 5 [32000/60000(53%)]\t Train Loss: 0.370686\n",
      "Train Epoch: 5 [38400/60000(64%)]\t Train Loss: 0.124575\n",
      "Train Epoch: 5 [44800/60000(75%)]\t Train Loss: 0.181009\n",
      "Train Epoch: 5 [51200/60000(85%)]\t Train Loss: 0.157205\n",
      "Train Epoch: 5 [57600/60000(96%)]\t Train Loss: 0.020448\n",
      "\n",
      "[Epoch: 5], \t Test Loss: 0.0025, \t Test Accuracy: 97.56 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\t Train Loss: 0.038796\n",
      "Train Epoch: 6 [6400/60000(11%)]\t Train Loss: 0.035962\n",
      "Train Epoch: 6 [12800/60000(21%)]\t Train Loss: 0.162037\n",
      "Train Epoch: 6 [19200/60000(32%)]\t Train Loss: 0.036685\n",
      "Train Epoch: 6 [25600/60000(43%)]\t Train Loss: 0.092803\n",
      "Train Epoch: 6 [32000/60000(53%)]\t Train Loss: 0.092760\n",
      "Train Epoch: 6 [38400/60000(64%)]\t Train Loss: 0.271783\n",
      "Train Epoch: 6 [44800/60000(75%)]\t Train Loss: 0.064261\n",
      "Train Epoch: 6 [51200/60000(85%)]\t Train Loss: 0.071045\n",
      "Train Epoch: 6 [57600/60000(96%)]\t Train Loss: 0.035683\n",
      "\n",
      "[Epoch: 6], \t Test Loss: 0.0024, \t Test Accuracy: 97.66 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\t Train Loss: 0.023497\n",
      "Train Epoch: 7 [6400/60000(11%)]\t Train Loss: 0.059282\n",
      "Train Epoch: 7 [12800/60000(21%)]\t Train Loss: 0.064435\n",
      "Train Epoch: 7 [19200/60000(32%)]\t Train Loss: 0.099721\n",
      "Train Epoch: 7 [25600/60000(43%)]\t Train Loss: 0.040713\n",
      "Train Epoch: 7 [32000/60000(53%)]\t Train Loss: 0.251160\n",
      "Train Epoch: 7 [38400/60000(64%)]\t Train Loss: 0.074482\n",
      "Train Epoch: 7 [44800/60000(75%)]\t Train Loss: 0.326525\n",
      "Train Epoch: 7 [51200/60000(85%)]\t Train Loss: 0.018105\n",
      "Train Epoch: 7 [57600/60000(96%)]\t Train Loss: 0.133029\n",
      "\n",
      "[Epoch: 7], \t Test Loss: 0.0023, \t Test Accuracy: 97.87 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\t Train Loss: 0.233176\n",
      "Train Epoch: 8 [6400/60000(11%)]\t Train Loss: 0.239577\n",
      "Train Epoch: 8 [12800/60000(21%)]\t Train Loss: 0.088585\n",
      "Train Epoch: 8 [19200/60000(32%)]\t Train Loss: 0.105093\n",
      "Train Epoch: 8 [25600/60000(43%)]\t Train Loss: 0.067169\n",
      "Train Epoch: 8 [32000/60000(53%)]\t Train Loss: 0.038811\n",
      "Train Epoch: 8 [38400/60000(64%)]\t Train Loss: 0.063795\n",
      "Train Epoch: 8 [44800/60000(75%)]\t Train Loss: 0.311885\n",
      "Train Epoch: 8 [51200/60000(85%)]\t Train Loss: 0.070506\n",
      "Train Epoch: 8 [57600/60000(96%)]\t Train Loss: 0.014322\n",
      "\n",
      "[Epoch: 8], \t Test Loss: 0.0021, \t Test Accuracy: 97.96 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\t Train Loss: 0.329391\n",
      "Train Epoch: 9 [6400/60000(11%)]\t Train Loss: 0.125534\n",
      "Train Epoch: 9 [12800/60000(21%)]\t Train Loss: 0.627312\n",
      "Train Epoch: 9 [19200/60000(32%)]\t Train Loss: 0.237360\n",
      "Train Epoch: 9 [25600/60000(43%)]\t Train Loss: 0.111839\n",
      "Train Epoch: 9 [32000/60000(53%)]\t Train Loss: 0.103002\n",
      "Train Epoch: 9 [38400/60000(64%)]\t Train Loss: 0.026781\n",
      "Train Epoch: 9 [44800/60000(75%)]\t Train Loss: 0.046963\n",
      "Train Epoch: 9 [51200/60000(85%)]\t Train Loss: 0.146171\n",
      "Train Epoch: 9 [57600/60000(96%)]\t Train Loss: 0.188129\n",
      "\n",
      "[Epoch: 9], \t Test Loss: 0.0021, \t Test Accuracy: 97.91 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\t Train Loss: 0.313304\n",
      "Train Epoch: 10 [6400/60000(11%)]\t Train Loss: 0.119086\n",
      "Train Epoch: 10 [12800/60000(21%)]\t Train Loss: 0.160558\n",
      "Train Epoch: 10 [19200/60000(32%)]\t Train Loss: 0.104519\n",
      "Train Epoch: 10 [25600/60000(43%)]\t Train Loss: 0.192942\n",
      "Train Epoch: 10 [32000/60000(53%)]\t Train Loss: 0.049243\n",
      "Train Epoch: 10 [38400/60000(64%)]\t Train Loss: 0.302189\n",
      "Train Epoch: 10 [44800/60000(75%)]\t Train Loss: 0.088388\n",
      "Train Epoch: 10 [51200/60000(85%)]\t Train Loss: 0.032008\n",
      "Train Epoch: 10 [57600/60000(96%)]\t Train Loss: 0.066044\n",
      "\n",
      "[Epoch: 10], \t Test Loss: 0.0021, \t Test Accuracy: 97.98 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## MLP 학습 -> Train, Test set의 Loss 및 Test Set Accuracy 확인\n",
    "\n",
    "for Epoch in range (1, EPOCHS +1 ):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   #train_loaer 학습데이터로 train model 실행, log_interval: 학습 과정 출력\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader) #각 epoch별로 출력되는 loss 값과 accuracy 계산\n",
    "    print(\"\\n[Epoch: {}], \\t Test Loss: {:.4f}, \\t Test Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}