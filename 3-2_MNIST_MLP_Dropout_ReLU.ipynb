{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Module Import\n",
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import torch # deep learning framework pytorch\n",
    "import torch.nn as nn # module including functions needed for deep learning/ai\n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using PyTorch version: 1.7.1 Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Identify device used for Deep Learning module\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = torch.device('cuda')\n",
    "else:\n",
    "  DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, 'Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 #모델 학습시 필요한 데이터 \n",
    "EPOCHS = 30 # batch 개 단위로 back propaagation을 이용해 MLP weight 업데이트 \n",
    "# hyperparameters usually capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST 데이터 다운로드 \n",
    "\n",
    "train_dataset = datasets.MNIST(root = \"data/MNIST\", #root: 데이터가 저장될 장소 지정\n",
    "                               train = True, #대상 데이터가 학습용\n",
    "                               download = True, #해당 데이터 인터넷에서 다운로드 \n",
    "                               transform = transforms.ToTensor()) #손글씨 '이미지'데이터 불러옴과 함께 전처리\n",
    "                               #tensor 형태로 데이터 불러옴, 0~255 범위의 픽셀 값 0~1 범위로 정규화\n",
    "test_dataset = datasets.MNIST(root = \"data/MNIST\", \n",
    "                              train = False, #대상 데이터가 검증용\n",
    "                              transform = transforms.ToTensor())\n",
    "#정규화해 불러온 train, test 데이터셋 mini-batch 단위로 분리해 지지정\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True) #데이터 순서 섞기\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type:  torch.FloatTensor\nY_train: torch.Size([32]) type:  torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "## 데이터 확인하기 -> 다운로드 한 후 mini-batch 단위로 할당한 데이터의 개수, 형태 확인\n",
    "\n",
    "for (X_train, y_train)in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type: ', X_train.type())\n",
    "    print('Y_train:', y_train.size(), 'type: ', y_train.type())\n",
    "    break\n",
    "\n",
    "# X_train: torch.Size([32, 1, 28, 28]) type:  torch.FloatTensor \n",
    "# --> 32개의 데이터로 batch 구성, 채널 1(그레이스케일), 28x28픽셀의 이미지\n",
    "# Y_train: torch.Size([32]) type:  torch.LongTensor\n",
    "# --> 32개의 이미지 데이터 각각에 label값 1개씩 존재, 32개의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP(Multi Layer Perceptron) 모델 설계\n",
    "\n",
    "class Net(nn.Module):   #PyTorch Moudle 내 딥러닝 모델 관련 기본 함수 포함하는 nn.Module 클래스 상속받는 Net 클래스 정의\n",
    "    def __init__(self): #Net 클래스의 instance 생성시 지니게 되는 성질 정의\n",
    "        super(Net, self).__init__() #nn.Module 내의 메서드 상속받아 이용 \n",
    "        self.fc1 = nn.Linear(28*28, 512)    #첫 번째 Fully Connected Layer 정의, input size: 28*28*1, output node 수: 512\n",
    "        self.fc2 = nn.Linear(512, 256)      #두 번째 Fully Connected Layer 정의, input: 512, output: 256 \n",
    "        self.fc3 = nn.Linear(256, 10)       #세 번째 Fully Connected Layer 정의, input: 256, output: 10 (0~9까지 10개의 클래스 표현 위한 라벨 one-hot-encoding)\n",
    "        self.dropout_prob = 0.5 #50%의 노드에 대해 가중값 계산하지 않음 \n",
    "\n",
    "    def forward(self, x):   #clss Net 이용해 설계한 MLP 모델의 Forward Propagation정의 -> output 계산하기까지 과정 나열\n",
    "        x = x.view(-1, 28*28)   #2차원 데이터 1차원 데이터로 변형, view 메서드 사용 - 784크기의 1차원 데이터로 변환(flatten)\n",
    "        x = self.fc1(x)     #첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터 통과\n",
    "        x = F.relu(x)    #torch.nn.functional 에 정의된 비선형 함수 relu() 이용해 fc2의 input 계산\n",
    "\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)   #relu()함수의 결과값에 dropout 적용\n",
    "\n",
    "        x = self.fc2(x)     #두 번째 Fully Connected Layer에서 relu() 함수로 계산된 결과값 통과\n",
    "        x = F.relu(x)    #torch.nn.functional 에 정의된 relu() 이용해 fc3의 input 값 계산\n",
    "\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "\n",
    "        x = self.fc3(x)     #세 번째 Fully Connected Layer에서 relu() 함수로 계산된 결과값 통과\n",
    "        x = F.log_softmax(x, dim = 1)   #torch.nn.functional 내의 log_softmax 이용해 최종 output 계산\n",
    "        #softmax 이용해 0~9까지 10개의 클래스에 속할 확률 값 계산 \n",
    "        #log_softmax -> 일반적인 softmax 보다 Back Propagation 이용해 학습시 loss 값에 대한 gradient값 보다 원활하게 계산(log 함수의 기울기 보다 부드럽게 변화)\n",
    "        \n",
    "        return x    #최종 계산된 x 값 output으로 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (fc1): Linear(in_features=784, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "## Optimizer, Objective Function 설정하기\n",
    "\n",
    "model = Net().to(DEVICE)    #앞서 정의한 모델을 DEVICE에 할당\n",
    "#Back Propagation 이용해 파라미터 업데이트시 이용하는 optimizer 정의\n",
    "#Stochastic Gradient Descent(SGD) 알고리즘 이용, Learning Rate 0.01, Momentum(Optimizer의 관성 나타냄) 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)  \n",
    "\n",
    "#MLP 모델의 output 값 + one-hot-encoding값과의 loss Criterion 이용해 계산 \n",
    "criterion = nn.CrossEntropyLoss()   #nn.CrossEntropyLoss() 로 criterion 설정\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Net(\n",
    "#   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
    "#   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
    "#   (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP 모델 학습 진행, 학습 데이터에 대한 모델 성능 확인\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()   #MLP 모델을 학습 상대로 지정\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):    #train_loader: 이미지+레이블 데이터 batch로 묶여서 저장\n",
    "                                                                #train_loader의 batch별로 저장된 데이터 순서대로 이용해 MLP 모델 학습시킴\n",
    "        image = image.to(DEVICE)    #mini-batch의 이미지 데이터 활용하기 위해 device에 할당\n",
    "        label = label.to(DEVICE)    #mini-batch의 레이블 데이터도 device에 할당\n",
    "        optimizer.zero_grad()       #기존의 device에 할당할 경우 과거에 이용한 데이터를 바탕으로 계산된 loss와 gradient값이 optimizer에 할당되어 있으므로 optimizer의 gradient값을 초기화\n",
    "        output = model(image)       #장비에 할당한 이미지 데이터를 MLP 모델의 input으로 output계산\n",
    "        loss = criterion(output, label) #계산된 output을 label값과 함께 기존에 정의한 cross entropy를 이용해 loss 계산\n",
    "        loss.backward()     #loss계산값을 바탕으로 back propagation -> 계산된 gradient 값 각 parameter에 할당\n",
    "        optimizer.step()    #각 파라미터에 할당된 graidient값을 이용해 파라미터 값 업데이트\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\t Train Loss: {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image),\n",
    "                len(train_loader.dataset), 100. *batch_idx / len(train_loader),\n",
    "                loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습되는 과정 중 검증 데이터에 대한 모델 성능을 확인\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()    #학습 과정, 학습 완료된 MLP 모델을 학습 상태가 아닌, 평가 상태로 지정\n",
    "    test_loss = 0   #test_loader 데이터를 이용해 loss 값 계산 위해 test_loss 0으로 임시설정\n",
    "    correct = 0     #학습 완료된 MLP 모델이 올바른 Class로 분류한 경우 세기 위한 변수\n",
    "\n",
    "    with torch.no_grad():   #torch.no_grad(): 모델 평가하는 단계에서 gradient통해 파라미터 값이 업데이트 되는 현상을 방지 \n",
    "        for image, label in test_loader:    #test_loader batch 단위로 저장 -> for 문으로 차례대로 접근\n",
    "            image = image.to(DEVICE)    #test_loader의 mini-batch내의 데이터 device에 할당\n",
    "            label = label.to(DEVICE)    #test_loader의 label 데이터 device에 할당\n",
    "            output = model(image)       #MLP 모델 output계산\n",
    "            test_loss += criterion(output, label).item()    #output과 label 값에 대해 CrossEntropy를 이용해 loss 계산-> test_loss 업데이트 \n",
    "            prediction = output.max(1, keepdim = True)[1]   #MLP 모델의 output -> 크기 10인 벡터값\n",
    "                                                            #계산된 벡터 값 내 가장 큰 값의 위치에 해당하는 클래스 예측\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()    #예측값 == label값일 때 correct 변수 1 증가(올바르게 예측한 횟수)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)   #현재까지의 test_loss를 batch 개수만큼 나누어 평균 loss 값 구함\n",
    "    test_accuracy = 100. *correct / len(test_loader.dataset)    #test_loader 데이터 중 얼마나 맞추었는지 정확도 계산\n",
    "    return test_loss, test_accuracy  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\t Train Loss: 2.301414\n",
      "Train Epoch: 1 [6400/60000(11%)]\t Train Loss: 2.055817\n",
      "Train Epoch: 1 [12800/60000(21%)]\t Train Loss: 1.233274\n",
      "Train Epoch: 1 [19200/60000(32%)]\t Train Loss: 0.754351\n",
      "Train Epoch: 1 [25600/60000(43%)]\t Train Loss: 0.628310\n",
      "Train Epoch: 1 [32000/60000(53%)]\t Train Loss: 0.462895\n",
      "Train Epoch: 1 [38400/60000(64%)]\t Train Loss: 0.533522\n",
      "Train Epoch: 1 [44800/60000(75%)]\t Train Loss: 0.596591\n",
      "Train Epoch: 1 [51200/60000(85%)]\t Train Loss: 0.294734\n",
      "Train Epoch: 1 [57600/60000(96%)]\t Train Loss: 0.454858\n",
      "\n",
      "[Epoch: 1], \t Test Loss: 0.0101, \t Test Accuracy: 90.79 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\t Train Loss: 0.411882\n",
      "Train Epoch: 2 [6400/60000(11%)]\t Train Loss: 0.313930\n",
      "Train Epoch: 2 [12800/60000(21%)]\t Train Loss: 0.205713\n",
      "Train Epoch: 2 [19200/60000(32%)]\t Train Loss: 0.298780\n",
      "Train Epoch: 2 [25600/60000(43%)]\t Train Loss: 0.396451\n",
      "Train Epoch: 2 [32000/60000(53%)]\t Train Loss: 0.114784\n",
      "Train Epoch: 2 [38400/60000(64%)]\t Train Loss: 0.521748\n",
      "Train Epoch: 2 [44800/60000(75%)]\t Train Loss: 0.315608\n",
      "Train Epoch: 2 [51200/60000(85%)]\t Train Loss: 0.332381\n",
      "Train Epoch: 2 [57600/60000(96%)]\t Train Loss: 0.208647\n",
      "\n",
      "[Epoch: 2], \t Test Loss: 0.0070, \t Test Accuracy: 93.49 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\t Train Loss: 0.251940\n",
      "Train Epoch: 3 [6400/60000(11%)]\t Train Loss: 0.308077\n",
      "Train Epoch: 3 [12800/60000(21%)]\t Train Loss: 0.080991\n",
      "Train Epoch: 3 [19200/60000(32%)]\t Train Loss: 0.144687\n",
      "Train Epoch: 3 [25600/60000(43%)]\t Train Loss: 0.281100\n",
      "Train Epoch: 3 [32000/60000(53%)]\t Train Loss: 0.183621\n",
      "Train Epoch: 3 [38400/60000(64%)]\t Train Loss: 0.535173\n",
      "Train Epoch: 3 [44800/60000(75%)]\t Train Loss: 0.507209\n",
      "Train Epoch: 3 [51200/60000(85%)]\t Train Loss: 0.262019\n",
      "Train Epoch: 3 [57600/60000(96%)]\t Train Loss: 0.163843\n",
      "\n",
      "[Epoch: 3], \t Test Loss: 0.0055, \t Test Accuracy: 94.72 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\t Train Loss: 0.091936\n",
      "Train Epoch: 4 [6400/60000(11%)]\t Train Loss: 0.332166\n",
      "Train Epoch: 4 [12800/60000(21%)]\t Train Loss: 0.268973\n",
      "Train Epoch: 4 [19200/60000(32%)]\t Train Loss: 0.134096\n",
      "Train Epoch: 4 [25600/60000(43%)]\t Train Loss: 0.127406\n",
      "Train Epoch: 4 [32000/60000(53%)]\t Train Loss: 0.223501\n",
      "Train Epoch: 4 [38400/60000(64%)]\t Train Loss: 0.356113\n",
      "Train Epoch: 4 [44800/60000(75%)]\t Train Loss: 0.206601\n",
      "Train Epoch: 4 [51200/60000(85%)]\t Train Loss: 0.725069\n",
      "Train Epoch: 4 [57600/60000(96%)]\t Train Loss: 0.267211\n",
      "\n",
      "[Epoch: 4], \t Test Loss: 0.0046, \t Test Accuracy: 95.60 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\t Train Loss: 0.256217\n",
      "Train Epoch: 5 [6400/60000(11%)]\t Train Loss: 0.197252\n",
      "Train Epoch: 5 [12800/60000(21%)]\t Train Loss: 0.377954\n",
      "Train Epoch: 5 [19200/60000(32%)]\t Train Loss: 0.095078\n",
      "Train Epoch: 5 [25600/60000(43%)]\t Train Loss: 0.175361\n",
      "Train Epoch: 5 [32000/60000(53%)]\t Train Loss: 0.155686\n",
      "Train Epoch: 5 [38400/60000(64%)]\t Train Loss: 0.379885\n",
      "Train Epoch: 5 [44800/60000(75%)]\t Train Loss: 0.308344\n",
      "Train Epoch: 5 [51200/60000(85%)]\t Train Loss: 0.205786\n",
      "Train Epoch: 5 [57600/60000(96%)]\t Train Loss: 0.271844\n",
      "\n",
      "[Epoch: 5], \t Test Loss: 0.0040, \t Test Accuracy: 96.18 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\t Train Loss: 0.159095\n",
      "Train Epoch: 6 [6400/60000(11%)]\t Train Loss: 0.090526\n",
      "Train Epoch: 6 [12800/60000(21%)]\t Train Loss: 0.342694\n",
      "Train Epoch: 6 [19200/60000(32%)]\t Train Loss: 0.321570\n",
      "Train Epoch: 6 [25600/60000(43%)]\t Train Loss: 0.028233\n",
      "Train Epoch: 6 [32000/60000(53%)]\t Train Loss: 0.278227\n",
      "Train Epoch: 6 [38400/60000(64%)]\t Train Loss: 0.182463\n",
      "Train Epoch: 6 [44800/60000(75%)]\t Train Loss: 0.103608\n",
      "Train Epoch: 6 [51200/60000(85%)]\t Train Loss: 0.265335\n",
      "Train Epoch: 6 [57600/60000(96%)]\t Train Loss: 0.118374\n",
      "\n",
      "[Epoch: 6], \t Test Loss: 0.0035, \t Test Accuracy: 96.53 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\t Train Loss: 0.149323\n",
      "Train Epoch: 7 [6400/60000(11%)]\t Train Loss: 0.253766\n",
      "Train Epoch: 7 [12800/60000(21%)]\t Train Loss: 0.084995\n",
      "Train Epoch: 7 [19200/60000(32%)]\t Train Loss: 0.159662\n",
      "Train Epoch: 7 [25600/60000(43%)]\t Train Loss: 0.207160\n",
      "Train Epoch: 7 [32000/60000(53%)]\t Train Loss: 0.063701\n",
      "Train Epoch: 7 [38400/60000(64%)]\t Train Loss: 0.370621\n",
      "Train Epoch: 7 [44800/60000(75%)]\t Train Loss: 0.222223\n",
      "Train Epoch: 7 [51200/60000(85%)]\t Train Loss: 0.033305\n",
      "Train Epoch: 7 [57600/60000(96%)]\t Train Loss: 0.039687\n",
      "\n",
      "[Epoch: 7], \t Test Loss: 0.0033, \t Test Accuracy: 96.78 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\t Train Loss: 0.102829\n",
      "Train Epoch: 8 [6400/60000(11%)]\t Train Loss: 0.062097\n",
      "Train Epoch: 8 [12800/60000(21%)]\t Train Loss: 0.141859\n",
      "Train Epoch: 8 [19200/60000(32%)]\t Train Loss: 0.211184\n",
      "Train Epoch: 8 [25600/60000(43%)]\t Train Loss: 0.064657\n",
      "Train Epoch: 8 [32000/60000(53%)]\t Train Loss: 0.056769\n",
      "Train Epoch: 8 [38400/60000(64%)]\t Train Loss: 0.176594\n",
      "Train Epoch: 8 [44800/60000(75%)]\t Train Loss: 0.054255\n",
      "Train Epoch: 8 [51200/60000(85%)]\t Train Loss: 0.088589\n",
      "Train Epoch: 8 [57600/60000(96%)]\t Train Loss: 0.108325\n",
      "\n",
      "[Epoch: 8], \t Test Loss: 0.0030, \t Test Accuracy: 97.15 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\t Train Loss: 0.107166\n",
      "Train Epoch: 9 [6400/60000(11%)]\t Train Loss: 0.025890\n",
      "Train Epoch: 9 [12800/60000(21%)]\t Train Loss: 0.016722\n",
      "Train Epoch: 9 [19200/60000(32%)]\t Train Loss: 0.097554\n",
      "Train Epoch: 9 [25600/60000(43%)]\t Train Loss: 0.134407\n",
      "Train Epoch: 9 [32000/60000(53%)]\t Train Loss: 0.013119\n",
      "Train Epoch: 9 [38400/60000(64%)]\t Train Loss: 0.069753\n",
      "Train Epoch: 9 [44800/60000(75%)]\t Train Loss: 0.115684\n",
      "Train Epoch: 9 [51200/60000(85%)]\t Train Loss: 0.178604\n",
      "Train Epoch: 9 [57600/60000(96%)]\t Train Loss: 0.047250\n",
      "\n",
      "[Epoch: 9], \t Test Loss: 0.0027, \t Test Accuracy: 97.27 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\t Train Loss: 0.036630\n",
      "Train Epoch: 10 [6400/60000(11%)]\t Train Loss: 0.072862\n",
      "Train Epoch: 10 [12800/60000(21%)]\t Train Loss: 0.114597\n",
      "Train Epoch: 10 [19200/60000(32%)]\t Train Loss: 0.129528\n",
      "Train Epoch: 10 [25600/60000(43%)]\t Train Loss: 0.089920\n",
      "Train Epoch: 10 [32000/60000(53%)]\t Train Loss: 0.125133\n",
      "Train Epoch: 10 [38400/60000(64%)]\t Train Loss: 0.244101\n",
      "Train Epoch: 10 [44800/60000(75%)]\t Train Loss: 0.159206\n",
      "Train Epoch: 10 [51200/60000(85%)]\t Train Loss: 0.075643\n",
      "Train Epoch: 10 [57600/60000(96%)]\t Train Loss: 0.041458\n",
      "\n",
      "[Epoch: 10], \t Test Loss: 0.0026, \t Test Accuracy: 97.34 %\n",
      "\n",
      "Train Epoch: 11 [0/60000(0%)]\t Train Loss: 0.062956\n",
      "Train Epoch: 11 [6400/60000(11%)]\t Train Loss: 0.362295\n",
      "Train Epoch: 11 [12800/60000(21%)]\t Train Loss: 0.281889\n",
      "Train Epoch: 11 [19200/60000(32%)]\t Train Loss: 0.093230\n",
      "Train Epoch: 11 [25600/60000(43%)]\t Train Loss: 0.081380\n",
      "Train Epoch: 11 [32000/60000(53%)]\t Train Loss: 0.156737\n",
      "Train Epoch: 11 [38400/60000(64%)]\t Train Loss: 0.229271\n",
      "Train Epoch: 11 [44800/60000(75%)]\t Train Loss: 0.078392\n",
      "Train Epoch: 11 [51200/60000(85%)]\t Train Loss: 0.205569\n",
      "Train Epoch: 11 [57600/60000(96%)]\t Train Loss: 0.115495\n",
      "\n",
      "[Epoch: 11], \t Test Loss: 0.0025, \t Test Accuracy: 97.64 %\n",
      "\n",
      "Train Epoch: 12 [0/60000(0%)]\t Train Loss: 0.172153\n",
      "Train Epoch: 12 [6400/60000(11%)]\t Train Loss: 0.107700\n",
      "Train Epoch: 12 [12800/60000(21%)]\t Train Loss: 0.062356\n",
      "Train Epoch: 12 [19200/60000(32%)]\t Train Loss: 0.026059\n",
      "Train Epoch: 12 [25600/60000(43%)]\t Train Loss: 0.045117\n",
      "Train Epoch: 12 [32000/60000(53%)]\t Train Loss: 0.132934\n",
      "Train Epoch: 12 [38400/60000(64%)]\t Train Loss: 0.093955\n",
      "Train Epoch: 12 [44800/60000(75%)]\t Train Loss: 0.016617\n",
      "Train Epoch: 12 [51200/60000(85%)]\t Train Loss: 0.098108\n",
      "Train Epoch: 12 [57600/60000(96%)]\t Train Loss: 0.041514\n",
      "\n",
      "[Epoch: 12], \t Test Loss: 0.0024, \t Test Accuracy: 97.56 %\n",
      "\n",
      "Train Epoch: 13 [0/60000(0%)]\t Train Loss: 0.022303\n",
      "Train Epoch: 13 [6400/60000(11%)]\t Train Loss: 0.042459\n",
      "Train Epoch: 13 [12800/60000(21%)]\t Train Loss: 0.019721\n",
      "Train Epoch: 13 [19200/60000(32%)]\t Train Loss: 0.178573\n",
      "Train Epoch: 13 [25600/60000(43%)]\t Train Loss: 0.047671\n",
      "Train Epoch: 13 [32000/60000(53%)]\t Train Loss: 0.094554\n",
      "Train Epoch: 13 [38400/60000(64%)]\t Train Loss: 0.019828\n",
      "Train Epoch: 13 [44800/60000(75%)]\t Train Loss: 0.140378\n",
      "Train Epoch: 13 [51200/60000(85%)]\t Train Loss: 0.006657\n",
      "Train Epoch: 13 [57600/60000(96%)]\t Train Loss: 0.041788\n",
      "\n",
      "[Epoch: 13], \t Test Loss: 0.0023, \t Test Accuracy: 97.68 %\n",
      "\n",
      "Train Epoch: 14 [0/60000(0%)]\t Train Loss: 0.430595\n",
      "Train Epoch: 14 [6400/60000(11%)]\t Train Loss: 0.016259\n",
      "Train Epoch: 14 [12800/60000(21%)]\t Train Loss: 0.154981\n",
      "Train Epoch: 14 [19200/60000(32%)]\t Train Loss: 0.080898\n",
      "Train Epoch: 14 [25600/60000(43%)]\t Train Loss: 0.017728\n",
      "Train Epoch: 14 [32000/60000(53%)]\t Train Loss: 0.105901\n",
      "Train Epoch: 14 [38400/60000(64%)]\t Train Loss: 0.114540\n",
      "Train Epoch: 14 [44800/60000(75%)]\t Train Loss: 0.061740\n",
      "Train Epoch: 14 [51200/60000(85%)]\t Train Loss: 0.070315\n",
      "Train Epoch: 14 [57600/60000(96%)]\t Train Loss: 0.049544\n",
      "\n",
      "[Epoch: 14], \t Test Loss: 0.0023, \t Test Accuracy: 97.82 %\n",
      "\n",
      "Train Epoch: 15 [0/60000(0%)]\t Train Loss: 0.055602\n",
      "Train Epoch: 15 [6400/60000(11%)]\t Train Loss: 0.008603\n",
      "Train Epoch: 15 [12800/60000(21%)]\t Train Loss: 0.090361\n",
      "Train Epoch: 15 [19200/60000(32%)]\t Train Loss: 0.181127\n",
      "Train Epoch: 15 [25600/60000(43%)]\t Train Loss: 0.154898\n",
      "Train Epoch: 15 [32000/60000(53%)]\t Train Loss: 0.031132\n",
      "Train Epoch: 15 [38400/60000(64%)]\t Train Loss: 0.090439\n",
      "Train Epoch: 15 [44800/60000(75%)]\t Train Loss: 0.092048\n",
      "Train Epoch: 15 [51200/60000(85%)]\t Train Loss: 0.021238\n",
      "Train Epoch: 15 [57600/60000(96%)]\t Train Loss: 0.026720\n",
      "\n",
      "[Epoch: 15], \t Test Loss: 0.0021, \t Test Accuracy: 97.80 %\n",
      "\n",
      "Train Epoch: 16 [0/60000(0%)]\t Train Loss: 0.077012\n",
      "Train Epoch: 16 [6400/60000(11%)]\t Train Loss: 0.039455\n",
      "Train Epoch: 16 [12800/60000(21%)]\t Train Loss: 0.122703\n",
      "Train Epoch: 16 [19200/60000(32%)]\t Train Loss: 0.025514\n",
      "Train Epoch: 16 [25600/60000(43%)]\t Train Loss: 0.215103\n",
      "Train Epoch: 16 [32000/60000(53%)]\t Train Loss: 0.028695\n",
      "Train Epoch: 16 [38400/60000(64%)]\t Train Loss: 0.135696\n",
      "Train Epoch: 16 [44800/60000(75%)]\t Train Loss: 0.123397\n",
      "Train Epoch: 16 [51200/60000(85%)]\t Train Loss: 0.174224\n",
      "Train Epoch: 16 [57600/60000(96%)]\t Train Loss: 0.078075\n",
      "\n",
      "[Epoch: 16], \t Test Loss: 0.0021, \t Test Accuracy: 97.92 %\n",
      "\n",
      "Train Epoch: 17 [0/60000(0%)]\t Train Loss: 0.054593\n",
      "Train Epoch: 17 [6400/60000(11%)]\t Train Loss: 0.047888\n",
      "Train Epoch: 17 [12800/60000(21%)]\t Train Loss: 0.020488\n",
      "Train Epoch: 17 [19200/60000(32%)]\t Train Loss: 0.027347\n",
      "Train Epoch: 17 [25600/60000(43%)]\t Train Loss: 0.120940\n",
      "Train Epoch: 17 [32000/60000(53%)]\t Train Loss: 0.098479\n",
      "Train Epoch: 17 [38400/60000(64%)]\t Train Loss: 0.066309\n",
      "Train Epoch: 17 [44800/60000(75%)]\t Train Loss: 0.053089\n",
      "Train Epoch: 17 [51200/60000(85%)]\t Train Loss: 0.103081\n",
      "Train Epoch: 17 [57600/60000(96%)]\t Train Loss: 0.013418\n",
      "\n",
      "[Epoch: 17], \t Test Loss: 0.0020, \t Test Accuracy: 97.94 %\n",
      "\n",
      "Train Epoch: 18 [0/60000(0%)]\t Train Loss: 0.112268\n",
      "Train Epoch: 18 [6400/60000(11%)]\t Train Loss: 0.192584\n",
      "Train Epoch: 18 [12800/60000(21%)]\t Train Loss: 0.149464\n",
      "Train Epoch: 18 [19200/60000(32%)]\t Train Loss: 0.016882\n",
      "Train Epoch: 18 [25600/60000(43%)]\t Train Loss: 0.056846\n",
      "Train Epoch: 18 [32000/60000(53%)]\t Train Loss: 0.059374\n",
      "Train Epoch: 18 [38400/60000(64%)]\t Train Loss: 0.038294\n",
      "Train Epoch: 18 [44800/60000(75%)]\t Train Loss: 0.127875\n",
      "Train Epoch: 18 [51200/60000(85%)]\t Train Loss: 0.025084\n",
      "Train Epoch: 18 [57600/60000(96%)]\t Train Loss: 0.015400\n",
      "\n",
      "[Epoch: 18], \t Test Loss: 0.0020, \t Test Accuracy: 98.07 %\n",
      "\n",
      "Train Epoch: 19 [0/60000(0%)]\t Train Loss: 0.028293\n",
      "Train Epoch: 19 [6400/60000(11%)]\t Train Loss: 0.029819\n",
      "Train Epoch: 19 [12800/60000(21%)]\t Train Loss: 0.049242\n",
      "Train Epoch: 19 [19200/60000(32%)]\t Train Loss: 0.031218\n",
      "Train Epoch: 19 [25600/60000(43%)]\t Train Loss: 0.124458\n",
      "Train Epoch: 19 [32000/60000(53%)]\t Train Loss: 0.137767\n",
      "Train Epoch: 19 [38400/60000(64%)]\t Train Loss: 0.096416\n",
      "Train Epoch: 19 [44800/60000(75%)]\t Train Loss: 0.219440\n",
      "Train Epoch: 19 [51200/60000(85%)]\t Train Loss: 0.022395\n",
      "Train Epoch: 19 [57600/60000(96%)]\t Train Loss: 0.019223\n",
      "\n",
      "[Epoch: 19], \t Test Loss: 0.0020, \t Test Accuracy: 97.96 %\n",
      "\n",
      "Train Epoch: 20 [0/60000(0%)]\t Train Loss: 0.040309\n",
      "Train Epoch: 20 [6400/60000(11%)]\t Train Loss: 0.060160\n",
      "Train Epoch: 20 [12800/60000(21%)]\t Train Loss: 0.050670\n",
      "Train Epoch: 20 [19200/60000(32%)]\t Train Loss: 0.150652\n",
      "Train Epoch: 20 [25600/60000(43%)]\t Train Loss: 0.021235\n",
      "Train Epoch: 20 [32000/60000(53%)]\t Train Loss: 0.083316\n",
      "Train Epoch: 20 [38400/60000(64%)]\t Train Loss: 0.055588\n",
      "Train Epoch: 20 [44800/60000(75%)]\t Train Loss: 0.020419\n",
      "Train Epoch: 20 [51200/60000(85%)]\t Train Loss: 0.018534\n",
      "Train Epoch: 20 [57600/60000(96%)]\t Train Loss: 0.034162\n",
      "\n",
      "[Epoch: 20], \t Test Loss: 0.0020, \t Test Accuracy: 98.11 %\n",
      "\n",
      "Train Epoch: 21 [0/60000(0%)]\t Train Loss: 0.016040\n",
      "Train Epoch: 21 [6400/60000(11%)]\t Train Loss: 0.040689\n",
      "Train Epoch: 21 [12800/60000(21%)]\t Train Loss: 0.019509\n",
      "Train Epoch: 21 [19200/60000(32%)]\t Train Loss: 0.022250\n",
      "Train Epoch: 21 [25600/60000(43%)]\t Train Loss: 0.014151\n",
      "Train Epoch: 21 [32000/60000(53%)]\t Train Loss: 0.134526\n",
      "Train Epoch: 21 [38400/60000(64%)]\t Train Loss: 0.098877\n",
      "Train Epoch: 21 [44800/60000(75%)]\t Train Loss: 0.344932\n",
      "Train Epoch: 21 [51200/60000(85%)]\t Train Loss: 0.209616\n",
      "Train Epoch: 21 [57600/60000(96%)]\t Train Loss: 0.038396\n",
      "\n",
      "[Epoch: 21], \t Test Loss: 0.0019, \t Test Accuracy: 98.04 %\n",
      "\n",
      "Train Epoch: 22 [0/60000(0%)]\t Train Loss: 0.045002\n",
      "Train Epoch: 22 [6400/60000(11%)]\t Train Loss: 0.165682\n",
      "Train Epoch: 22 [12800/60000(21%)]\t Train Loss: 0.021853\n",
      "Train Epoch: 22 [19200/60000(32%)]\t Train Loss: 0.197011\n",
      "Train Epoch: 22 [25600/60000(43%)]\t Train Loss: 0.009217\n",
      "Train Epoch: 22 [32000/60000(53%)]\t Train Loss: 0.028066\n",
      "Train Epoch: 22 [38400/60000(64%)]\t Train Loss: 0.024621\n",
      "Train Epoch: 22 [44800/60000(75%)]\t Train Loss: 0.019248\n",
      "Train Epoch: 22 [51200/60000(85%)]\t Train Loss: 0.161203\n",
      "Train Epoch: 22 [57600/60000(96%)]\t Train Loss: 0.017858\n",
      "\n",
      "[Epoch: 22], \t Test Loss: 0.0019, \t Test Accuracy: 98.05 %\n",
      "\n",
      "Train Epoch: 23 [0/60000(0%)]\t Train Loss: 0.008584\n",
      "Train Epoch: 23 [6400/60000(11%)]\t Train Loss: 0.084191\n",
      "Train Epoch: 23 [12800/60000(21%)]\t Train Loss: 0.044572\n",
      "Train Epoch: 23 [19200/60000(32%)]\t Train Loss: 0.102206\n",
      "Train Epoch: 23 [25600/60000(43%)]\t Train Loss: 0.009726\n",
      "Train Epoch: 23 [32000/60000(53%)]\t Train Loss: 0.056626\n",
      "Train Epoch: 23 [38400/60000(64%)]\t Train Loss: 0.019984\n",
      "Train Epoch: 23 [44800/60000(75%)]\t Train Loss: 0.044778\n",
      "Train Epoch: 23 [51200/60000(85%)]\t Train Loss: 0.075984\n",
      "Train Epoch: 23 [57600/60000(96%)]\t Train Loss: 0.072396\n",
      "\n",
      "[Epoch: 23], \t Test Loss: 0.0019, \t Test Accuracy: 98.04 %\n",
      "\n",
      "Train Epoch: 24 [0/60000(0%)]\t Train Loss: 0.070116\n",
      "Train Epoch: 24 [6400/60000(11%)]\t Train Loss: 0.017028\n",
      "Train Epoch: 24 [12800/60000(21%)]\t Train Loss: 0.025298\n",
      "Train Epoch: 24 [19200/60000(32%)]\t Train Loss: 0.225381\n",
      "Train Epoch: 24 [25600/60000(43%)]\t Train Loss: 0.016152\n",
      "Train Epoch: 24 [32000/60000(53%)]\t Train Loss: 0.130698\n",
      "Train Epoch: 24 [38400/60000(64%)]\t Train Loss: 0.020647\n",
      "Train Epoch: 24 [44800/60000(75%)]\t Train Loss: 0.080363\n",
      "Train Epoch: 24 [51200/60000(85%)]\t Train Loss: 0.004974\n",
      "Train Epoch: 24 [57600/60000(96%)]\t Train Loss: 0.006767\n",
      "\n",
      "[Epoch: 24], \t Test Loss: 0.0018, \t Test Accuracy: 98.19 %\n",
      "\n",
      "Train Epoch: 25 [0/60000(0%)]\t Train Loss: 0.031647\n",
      "Train Epoch: 25 [6400/60000(11%)]\t Train Loss: 0.090132\n",
      "Train Epoch: 25 [12800/60000(21%)]\t Train Loss: 0.132164\n",
      "Train Epoch: 25 [19200/60000(32%)]\t Train Loss: 0.022543\n",
      "Train Epoch: 25 [25600/60000(43%)]\t Train Loss: 0.051591\n",
      "Train Epoch: 25 [32000/60000(53%)]\t Train Loss: 0.087843\n",
      "Train Epoch: 25 [38400/60000(64%)]\t Train Loss: 0.094645\n",
      "Train Epoch: 25 [44800/60000(75%)]\t Train Loss: 0.140143\n",
      "Train Epoch: 25 [51200/60000(85%)]\t Train Loss: 0.012450\n",
      "Train Epoch: 25 [57600/60000(96%)]\t Train Loss: 0.011513\n",
      "\n",
      "[Epoch: 25], \t Test Loss: 0.0018, \t Test Accuracy: 98.15 %\n",
      "\n",
      "Train Epoch: 26 [0/60000(0%)]\t Train Loss: 0.046648\n",
      "Train Epoch: 26 [6400/60000(11%)]\t Train Loss: 0.037690\n",
      "Train Epoch: 26 [12800/60000(21%)]\t Train Loss: 0.075242\n",
      "Train Epoch: 26 [19200/60000(32%)]\t Train Loss: 0.032284\n",
      "Train Epoch: 26 [25600/60000(43%)]\t Train Loss: 0.115407\n",
      "Train Epoch: 26 [32000/60000(53%)]\t Train Loss: 0.044060\n",
      "Train Epoch: 26 [38400/60000(64%)]\t Train Loss: 0.026830\n",
      "Train Epoch: 26 [44800/60000(75%)]\t Train Loss: 0.025462\n",
      "Train Epoch: 26 [51200/60000(85%)]\t Train Loss: 0.087195\n",
      "Train Epoch: 26 [57600/60000(96%)]\t Train Loss: 0.069313\n",
      "\n",
      "[Epoch: 26], \t Test Loss: 0.0018, \t Test Accuracy: 98.19 %\n",
      "\n",
      "Train Epoch: 27 [0/60000(0%)]\t Train Loss: 0.180236\n",
      "Train Epoch: 27 [6400/60000(11%)]\t Train Loss: 0.033950\n",
      "Train Epoch: 27 [12800/60000(21%)]\t Train Loss: 0.016910\n",
      "Train Epoch: 27 [19200/60000(32%)]\t Train Loss: 0.003794\n",
      "Train Epoch: 27 [25600/60000(43%)]\t Train Loss: 0.105505\n",
      "Train Epoch: 27 [32000/60000(53%)]\t Train Loss: 0.008691\n",
      "Train Epoch: 27 [38400/60000(64%)]\t Train Loss: 0.054464\n",
      "Train Epoch: 27 [44800/60000(75%)]\t Train Loss: 0.024927\n",
      "Train Epoch: 27 [51200/60000(85%)]\t Train Loss: 0.020362\n",
      "Train Epoch: 27 [57600/60000(96%)]\t Train Loss: 0.025307\n",
      "\n",
      "[Epoch: 27], \t Test Loss: 0.0018, \t Test Accuracy: 98.23 %\n",
      "\n",
      "Train Epoch: 28 [0/60000(0%)]\t Train Loss: 0.046946\n",
      "Train Epoch: 28 [6400/60000(11%)]\t Train Loss: 0.016549\n",
      "Train Epoch: 28 [12800/60000(21%)]\t Train Loss: 0.012750\n",
      "Train Epoch: 28 [19200/60000(32%)]\t Train Loss: 0.058464\n",
      "Train Epoch: 28 [25600/60000(43%)]\t Train Loss: 0.056043\n",
      "Train Epoch: 28 [32000/60000(53%)]\t Train Loss: 0.008876\n",
      "Train Epoch: 28 [38400/60000(64%)]\t Train Loss: 0.032830\n",
      "Train Epoch: 28 [44800/60000(75%)]\t Train Loss: 0.013273\n",
      "Train Epoch: 28 [51200/60000(85%)]\t Train Loss: 0.004897\n",
      "Train Epoch: 28 [57600/60000(96%)]\t Train Loss: 0.069148\n",
      "\n",
      "[Epoch: 28], \t Test Loss: 0.0017, \t Test Accuracy: 98.33 %\n",
      "\n",
      "Train Epoch: 29 [0/60000(0%)]\t Train Loss: 0.039161\n",
      "Train Epoch: 29 [6400/60000(11%)]\t Train Loss: 0.007159\n",
      "Train Epoch: 29 [12800/60000(21%)]\t Train Loss: 0.016892\n",
      "Train Epoch: 29 [19200/60000(32%)]\t Train Loss: 0.053318\n",
      "Train Epoch: 29 [25600/60000(43%)]\t Train Loss: 0.012156\n",
      "Train Epoch: 29 [32000/60000(53%)]\t Train Loss: 0.005172\n",
      "Train Epoch: 29 [38400/60000(64%)]\t Train Loss: 0.041165\n",
      "Train Epoch: 29 [44800/60000(75%)]\t Train Loss: 0.010302\n",
      "Train Epoch: 29 [51200/60000(85%)]\t Train Loss: 0.023498\n",
      "Train Epoch: 29 [57600/60000(96%)]\t Train Loss: 0.019145\n",
      "\n",
      "[Epoch: 29], \t Test Loss: 0.0017, \t Test Accuracy: 98.29 %\n",
      "\n",
      "Train Epoch: 30 [0/60000(0%)]\t Train Loss: 0.074727\n",
      "Train Epoch: 30 [6400/60000(11%)]\t Train Loss: 0.011056\n",
      "Train Epoch: 30 [12800/60000(21%)]\t Train Loss: 0.034429\n",
      "Train Epoch: 30 [19200/60000(32%)]\t Train Loss: 0.065214\n",
      "Train Epoch: 30 [25600/60000(43%)]\t Train Loss: 0.005960\n",
      "Train Epoch: 30 [32000/60000(53%)]\t Train Loss: 0.421192\n",
      "Train Epoch: 30 [38400/60000(64%)]\t Train Loss: 0.184894\n",
      "Train Epoch: 30 [44800/60000(75%)]\t Train Loss: 0.056043\n",
      "Train Epoch: 30 [51200/60000(85%)]\t Train Loss: 0.021732\n",
      "Train Epoch: 30 [57600/60000(96%)]\t Train Loss: 0.031345\n",
      "\n",
      "[Epoch: 30], \t Test Loss: 0.0017, \t Test Accuracy: 98.37 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## MLP 학습 -> Train, Test set의 Loss 및 Test Set Accuracy 확인\n",
    "\n",
    "for Epoch in range (1, EPOCHS +1 ):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   #train_loaer 학습데이터로 train model 실행, log_interval: 학습 과정 출력\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader) #각 epoch별로 출력되는 loss 값과 accuracy 계산\n",
    "    print(\"\\n[Epoch: {}], \\t Test Loss: {:.4f}, \\t Test Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}