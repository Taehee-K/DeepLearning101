{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd06d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Module Import\n",
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import torch # deep learning framework pytorch\n",
    "import torch.nn as nn # module including functions needed for deep learning/ai\n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using PyTorch version: 1.7.1 Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Identify device used for Deep Learning module\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = torch.device('cuda')\n",
    "else:\n",
    "  DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, 'Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 #모델 학습시 필요한 데이터 \n",
    "EPOCHS = 30 # batch 개 단위로 back propaagation을 이용해 MLP weight 업데이트 \n",
    "# hyperparameters usually capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST 데이터 다운로드 \n",
    "\n",
    "train_dataset = datasets.MNIST(root = \"data/MNIST\", #root: 데이터가 저장될 장소 지정\n",
    "                               train = True, #대상 데이터가 학습용\n",
    "                               download = True, #해당 데이터 인터넷에서 다운로드 \n",
    "                               transform = transforms.ToTensor()) #손글씨 '이미지'데이터 불러옴과 함께 전처리\n",
    "                               #tensor 형태로 데이터 불러옴, 0~255 범위의 픽셀 값 0~1 범위로 정규화\n",
    "test_dataset = datasets.MNIST(root = \"data/MNIST\", \n",
    "                              train = False, #대상 데이터가 검증용\n",
    "                              transform = transforms.ToTensor())\n",
    "#정규화해 불러온 train, test 데이터셋 mini-batch 단위로 분리해 지지정\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True) #데이터 순서 섞기\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type:  torch.FloatTensor\nY_train: torch.Size([32]) type:  torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "## 데이터 확인하기 -> 다운로드 한 후 mini-batch 단위로 할당한 데이터의 개수, 형태 확인\n",
    "\n",
    "for (X_train, y_train)in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type: ', X_train.type())\n",
    "    print('Y_train:', y_train.size(), 'type: ', y_train.type())\n",
    "    break\n",
    "\n",
    "# X_train: torch.Size([32, 1, 28, 28]) type:  torch.FloatTensor \n",
    "# --> 32개의 데이터로 batch 구성, 채널 1(그레이스케일), 28x28픽셀의 이미지\n",
    "# Y_train: torch.Size([32]) type:  torch.LongTensor\n",
    "# --> 32개의 이미지 데이터 각각에 label값 1개씩 존재, 32개의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP(Multi Layer Perceptron) 모델 설계\n",
    "\n",
    "class Net(nn.Module):   #PyTorch Moudle 내 딥러닝 모델 관련 기본 함수 포함하는 nn.Module 클래스 상속받는 Net 클래스 정의\n",
    "    def __init__(self): #Net 클래스의 instance 생성시 지니게 되는 성질 정의\n",
    "        super(Net, self).__init__() #nn.Module 내의 메서드 상속받아 이용 \n",
    "        self.fc1 = nn.Linear(28*28, 512)    #첫 번째 Fully Connected Layer 정의, input size: 28*28*1, output node 수: 512\n",
    "        self.fc2 = nn.Linear(512, 256)      #두 번째 Fully Connected Layer 정의, input: 512, output: 256 \n",
    "        self.fc3 = nn.Linear(256, 10)       #세 번째 Fully Connected Layer 정의, input: 256, output: 10 (0~9까지 10개의 클래스 표현 위한 라벨 one-hot-encoding)\n",
    "        self.dropout_prob = 0.5 #50%의 노드에 대해 가중값 계산하지 않음 \n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)  #nn.BatchNorm1d() -> 1차원 벡터 batch normalization \n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    # Batch Normalization 적용하는 부분 -> 논문에 따라 activation function 이전/이후 적용 선호도 갈림\n",
    "\n",
    "    def forward(self, x):   #clss Net 이용해 설계한 MLP 모델의 Forward Propagation정의 -> output 계산하기까지 과정 나열\n",
    "        x = x.view(-1, 28*28)   #2차원 데이터 1차원 데이터로 변형, view 메서드 사용 - 784크기의 1차원 데이터로 변환(flatten)\n",
    "        x = self.fc1(x)     #첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터 통과\n",
    "        x = self.batch_norm1(x) #activation func 이전에 batch normalization 진행\n",
    "        x = F.relu(x)    #torch.nn.functional 에 정의된 비선형 함수 relu() 이용해 fc2의 input 계산\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)   #relu()함수의 결과값에 dropout 적용\n",
    "\n",
    "        x = self.fc2(x)     #두 번째 Fully Connected Layer에서 relu() 함수로 계산된 결과값 통과\n",
    "        x = self.batch_norm2(x) #두 번째 Fully Connected Layer의 output batch normalization\n",
    "        x = F.relu(x)    #torch.nn.functional 에 정의된 relu() 이용해 fc3의 input 값 계산\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "\n",
    "        x = self.fc3(x)     #세 번째 Fully Connected Layer에서 relu() 함수로 계산된 결과값 통과\n",
    "        x = F.log_softmax(x, dim = 1)   #torch.nn.functional 내의 log_softmax 이용해 최종 output 계산\n",
    "        #softmax 이용해 0~9까지 10개의 클래스에 속할 확률 값 계산 \n",
    "        #log_softmax -> 일반적인 softmax 보다 Back Propagation 이용해 학습시 loss 값에 대한 gradient값 보다 원활하게 계산(log 함수의 기울기 보다 부드럽게 변화)\n",
    "        \n",
    "        return x    #최종 계산된 x 값 output으로 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (fc1): Linear(in_features=784, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n"
     ]
    }
   ],
   "source": [
    "## Optimizer, Objective Function 설정하기\n",
    "\n",
    "import torch.nn.init as init    #weight, bias 등 딥러닝 모델에서 초기값으로 설정되는 요소에 대한 모듈 init import\n",
    "def weight_init(m): #weight 초기화할 부분 설정하는 weight_init 함수\n",
    "    if isinstance(m, nn.Linear):    #MLP 파라미터 중 nn.Linear에 해당하는 파라미터들에 대해서만\n",
    "        init.kaiming_uniform_(m.weight.data)    #nn.Linear 파라미터들에 대해 he_initialization 이용해 파라미터 초기화\n",
    "        #kaiming_uniform_ : 'He Initialization'\n",
    "\n",
    "model = Net().to(DEVICE)    #앞서 정의한 모델을 DEVICE에 할당\n",
    "model.apply(weight_init)    #모델 weight 초기화\n",
    "\n",
    "#Back Propagation 이용해 파라미터 업데이트시 이용하는 optimizer 정의\n",
    "#Stochastic Gradient Descent(SGD) 알고리즘 이용, Learning Rate 0.01, Momentum(Optimizer의 관성 나타냄) 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)  \n",
    "\n",
    "#MLP 모델의 output 값 + one-hot-encoding값과의 loss Criterion 이용해 계산 \n",
    "criterion = nn.CrossEntropyLoss()   #nn.CrossEntropyLoss() 로 criterion 설정\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Net(\n",
    "#   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
    "#   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
    "#   (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP 모델 학습 진행, 학습 데이터에 대한 모델 성능 확인\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()   #MLP 모델을 학습 상대로 지정\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):    #train_loader: 이미지+레이블 데이터 batch로 묶여서 저장\n",
    "                                                                #train_loader의 batch별로 저장된 데이터 순서대로 이용해 MLP 모델 학습시킴\n",
    "        image = image.to(DEVICE)    #mini-batch의 이미지 데이터 활용하기 위해 device에 할당\n",
    "        label = label.to(DEVICE)    #mini-batch의 레이블 데이터도 device에 할당\n",
    "        optimizer.zero_grad()       #기존의 device에 할당할 경우 과거에 이용한 데이터를 바탕으로 계산된 loss와 gradient값이 optimizer에 할당되어 있으므로 optimizer의 gradient값을 초기화\n",
    "        output = model(image)       #장비에 할당한 이미지 데이터를 MLP 모델의 input으로 output계산\n",
    "        loss = criterion(output, label) #계산된 output을 label값과 함께 기존에 정의한 cross entropy를 이용해 loss 계산\n",
    "        loss.backward()     #loss계산값을 바탕으로 back propagation -> 계산된 gradient 값 각 parameter에 할당\n",
    "        optimizer.step()    #각 파라미터에 할당된 graidient값을 이용해 파라미터 값 업데이트\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\t Train Loss: {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image),\n",
    "                len(train_loader.dataset), 100. *batch_idx / len(train_loader),\n",
    "                loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습되는 과정 중 검증 데이터에 대한 모델 성능을 확인\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()    #학습 과정, 학습 완료된 MLP 모델을 학습 상태가 아닌, 평가 상태로 지정\n",
    "    test_loss = 0   #test_loader 데이터를 이용해 loss 값 계산 위해 test_loss 0으로 임시설정\n",
    "    correct = 0     #학습 완료된 MLP 모델이 올바른 Class로 분류한 경우 세기 위한 변수\n",
    "\n",
    "    with torch.no_grad():   #torch.no_grad(): 모델 평가하는 단계에서 gradient통해 파라미터 값이 업데이트 되는 현상을 방지 \n",
    "        for image, label in test_loader:    #test_loader batch 단위로 저장 -> for 문으로 차례대로 접근\n",
    "            image = image.to(DEVICE)    #test_loader의 mini-batch내의 데이터 device에 할당\n",
    "            label = label.to(DEVICE)    #test_loader의 label 데이터 device에 할당\n",
    "            output = model(image)       #MLP 모델 output계산\n",
    "            test_loss += criterion(output, label).item()    #output과 label 값에 대해 CrossEntropy를 이용해 loss 계산-> test_loss 업데이트 \n",
    "            prediction = output.max(1, keepdim = True)[1]   #MLP 모델의 output -> 크기 10인 벡터값\n",
    "                                                            #계산된 벡터 값 내 가장 큰 값의 위치에 해당하는 클래스 예측\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()    #예측값 == label값일 때 correct 변수 1 증가(올바르게 예측한 횟수)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)   #현재까지의 test_loss를 batch 개수만큼 나누어 평균 loss 값 구함\n",
    "    test_accuracy = 100. *correct / len(test_loader.dataset)    #test_loader 데이터 중 얼마나 맞추었는지 정확도 계산\n",
    "    return test_loss, test_accuracy  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "160959\n",
      "Train Epoch: 27 [57600/60000(96%)]\t Train Loss: 0.041112\n",
      "\n",
      "[Epoch: 27], \t Test Loss: 0.0019, \t Test Accuracy: 98.13 %\n",
      "\n",
      "Train Epoch: 28 [0/60000(0%)]\t Train Loss: 0.379572\n",
      "Train Epoch: 28 [6400/60000(11%)]\t Train Loss: 0.067753\n",
      "Train Epoch: 28 [12800/60000(21%)]\t Train Loss: 0.143492\n",
      "Train Epoch: 28 [19200/60000(32%)]\t Train Loss: 0.082042\n",
      "Train Epoch: 28 [25600/60000(43%)]\t Train Loss: 0.218574\n",
      "Train Epoch: 28 [32000/60000(53%)]\t Train Loss: 0.016512\n",
      "Train Epoch: 28 [38400/60000(64%)]\t Train Loss: 0.072988\n",
      "Train Epoch: 28 [44800/60000(75%)]\t Train Loss: 0.092662\n",
      "Train Epoch: 28 [51200/60000(85%)]\t Train Loss: 0.022797\n",
      "Train Epoch: 28 [57600/60000(96%)]\t Train Loss: 0.035118\n",
      "\n",
      "[Epoch: 28], \t Test Loss: 0.0019, \t Test Accuracy: 98.13 %\n",
      "\n",
      "Train Epoch: 29 [0/60000(0%)]\t Train Loss: 0.264779\n",
      "Train Epoch: 29 [6400/60000(11%)]\t Train Loss: 0.097841\n",
      "Train Epoch: 29 [12800/60000(21%)]\t Train Loss: 0.036285\n",
      "Train Epoch: 29 [19200/60000(32%)]\t Train Loss: 0.005401\n",
      "Train Epoch: 29 [25600/60000(43%)]\t Train Loss: 0.452036\n",
      "Train Epoch: 29 [32000/60000(53%)]\t Train Loss: 0.024456\n",
      "Train Epoch: 29 [38400/60000(64%)]\t Train Loss: 0.025644\n",
      "Train Epoch: 29 [44800/60000(75%)]\t Train Loss: 0.036527\n",
      "Train Epoch: 29 [51200/60000(85%)]\t Train Loss: 0.010573\n",
      "Train Epoch: 29 [57600/60000(96%)]\t Train Loss: 0.161848\n",
      "\n",
      "[Epoch: 29], \t Test Loss: 0.0018, \t Test Accuracy: 98.19 %\n",
      "\n",
      "Train Epoch: 30 [0/60000(0%)]\t Train Loss: 0.043703\n",
      "Train Epoch: 30 [6400/60000(11%)]\t Train Loss: 0.237994\n",
      "Train Epoch: 30 [12800/60000(21%)]\t Train Loss: 0.096424\n",
      "Train Epoch: 30 [19200/60000(32%)]\t Train Loss: 0.080515\n",
      "Train Epoch: 30 [25600/60000(43%)]\t Train Loss: 0.087242\n",
      "Train Epoch: 30 [32000/60000(53%)]\t Train Loss: 0.037960\n",
      "Train Epoch: 30 [38400/60000(64%)]\t Train Loss: 0.107048\n",
      "Train Epoch: 30 [44800/60000(75%)]\t Train Loss: 0.020574\n",
      "Train Epoch: 30 [51200/60000(85%)]\t Train Loss: 0.332941\n",
      "Train Epoch: 30 [57600/60000(96%)]\t Train Loss: 0.045446\n",
      "\n",
      "[Epoch: 30], \t Test Loss: 0.0018, \t Test Accuracy: 98.14 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## MLP 학습 -> Train, Test set의 Loss 및 Test Set Accuracy 확인\n",
    "\n",
    "for Epoch in range (1, EPOCHS +1 ):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   #train_loaer 학습데이터로 train model 실행, log_interval: 학습 과정 출력\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader) #각 epoch별로 출력되는 loss 값과 accuracy 계산\n",
    "    print(\"\\n[Epoch: {}], \\t Test Loss: {:.4f}, \\t Test Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}