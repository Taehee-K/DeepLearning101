{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Module Import\n",
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import torch # deep learning framework pytorch\n",
    "import torch.nn as nn # module including functions needed for deep learning/ai\n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using PyTorch version: 1.7.1 Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## Identify device used for Deep Learning module\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = torch.device('cuda')\n",
    "else:\n",
    "  DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, 'Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 #모델 학습시 필요한 데이터 \n",
    "EPOCHS = 30 # batch 개 단위로 back propaagation을 이용해 MLP weight 업데이트 \n",
    "# hyperparameters usually capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST 데이터 다운로드 \n",
    "\n",
    "train_dataset = datasets.MNIST(root = \"data/MNIST\", #root: 데이터가 저장될 장소 지정\n",
    "                               train = True, #대상 데이터가 학습용\n",
    "                               download = True, #해당 데이터 인터넷에서 다운로드 \n",
    "                               transform = transforms.ToTensor()) #손글씨 '이미지'데이터 불러옴과 함께 전처리\n",
    "                               #tensor 형태로 데이터 불러옴, 0~255 범위의 픽셀 값 0~1 범위로 정규화\n",
    "test_dataset = datasets.MNIST(root = \"data/MNIST\", \n",
    "                              train = False, #대상 데이터가 검증용\n",
    "                              transform = transforms.ToTensor())\n",
    "#정규화해 불러온 train, test 데이터셋 mini-batch 단위로 분리해 지지정\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True) #데이터 순서 섞기\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type:  torch.FloatTensor\nY_train: torch.Size([32]) type:  torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "## 데이터 확인하기 -> 다운로드 한 후 mini-batch 단위로 할당한 데이터의 개수, 형태 확인\n",
    "\n",
    "for (X_train, y_train)in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type: ', X_train.type())\n",
    "    print('Y_train:', y_train.size(), 'type: ', y_train.type())\n",
    "    break\n",
    "\n",
    "# X_train: torch.Size([32, 1, 28, 28]) type:  torch.FloatTensor \n",
    "# --> 32개의 데이터로 batch 구성, 채널 1(그레이스케일), 28x28픽셀의 이미지\n",
    "# Y_train: torch.Size([32]) type:  torch.LongTensor\n",
    "# --> 32개의 이미지 데이터 각각에 label값 1개씩 존재, 32개의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP(Multi Layer Perceptron) 모델 설계\n",
    "\n",
    "class Net(nn.Module):   #PyTorch Moudle 내 딥러닝 모델 관련 기본 함수 포함하는 nn.Module 클래스 상속받는 Net 클래스 정의\n",
    "    def __init__(self): #Net 클래스의 instance 생성시 지니게 되는 성질 정의\n",
    "        super(Net, self).__init__() #nn.Module 내의 메서드 상속받아 이용 \n",
    "        self.fc1 = nn.Linear(28*28, 512)    #첫 번째 Fully Connected Layer 정의, input size: 28*28*1, output node 수: 512\n",
    "        self.fc2 = nn.Linear(512, 256)      #두 번째 Fully Connected Layer 정의, input: 512, output: 256 \n",
    "        self.fc3 = nn.Linear(256, 10)       #세 번째 Fully Connected Layer 정의, input: 256, output: 10 (0~9까지 10개의 클래스 표현 위한 라벨 one-hot-encoding)\n",
    "        self.dropout_prob = 0.5 #50%의 노드에 대해 가중값 계산하지 않음 \n",
    "\n",
    "    def forward(self, x):   #clss Net 이용해 설계한 MLP 모델의 Forward Propagation정의 -> output 계산하기까지 과정 나열\n",
    "        x = x.view(-1, 28*28)   #2차원 데이터 1차원 데이터로 변형, view 메서드 사용 - 784크기의 1차원 데이터로 변환(flatten)\n",
    "        x = self.fc1(x)     #첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터 통과\n",
    "        x = F.sigmoid(x)    #torch.nn.functional 에 정의된 비선형 함수 sigmoid() 이용해 fc2의 input 계산\n",
    "\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)   #sigmoid()함수의 결과값에 dropout 적용\n",
    "\n",
    "        x = self.fc2(x)     #두 번째 Fully Connected Layer에서 sigmoid() 함수로 계산된 결과값 통과\n",
    "        x = F.sigmoid(x)    #torch.nn.functional 에 정의된 sigmoid() 이용해 fc3의 input 값 계산\n",
    "\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "\n",
    "        x = self.fc3(x)     #세 번째 Fully Connected Layer에서 sigmoid() 함수로 계산된 결과값 통과\n",
    "        x = F.log_softmax(x, dim = 1)   #torch.nn.functional 내의 log_softmax 이용해 최종 output 계산\n",
    "        #softmax 이용해 0~9까지 10개의 클래스에 속할 확률 값 계산 \n",
    "        #log_softmax -> 일반적인 softmax 보다 Back Propagation 이용해 학습시 loss 값에 대한 gradient값 보다 원활하게 계산(log 함수의 기울기 보다 부드럽게 변화)\n",
    "        \n",
    "        return x    #최종 계산된 x 값 output으로 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (fc1): Linear(in_features=784, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "## Optimizer, Objective Function 설정하기\n",
    "\n",
    "model = Net().to(DEVICE)    #앞서 정의한 모델을 DEVICE에 할당\n",
    "#Back Propagation 이용해 파라미터 업데이트시 이용하는 optimizer 정의\n",
    "#Stochastic Gradient Descent(SGD) 알고리즘 이용, Learning Rate 0.01, Momentum(Optimizer의 관성 나타냄) 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)  \n",
    "\n",
    "#MLP 모델의 output 값 + one-hot-encoding값과의 loss Criterion 이용해 계산 \n",
    "criterion = nn.CrossEntropyLoss()   #nn.CrossEntropyLoss() 로 criterion 설정\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Net(\n",
    "#   (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
    "#   (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
    "#   (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP 모델 학습 진행, 학습 데이터에 대한 모델 성능 확인\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()   #MLP 모델을 학습 상대로 지정\n",
    "    for batch_idx,(image, label) in enumerate(train_loader):    #train_loader: 이미지+레이블 데이터 batch로 묶여서 저장\n",
    "                                                                #train_loader의 batch별로 저장된 데이터 순서대로 이용해 MLP 모델 학습시킴\n",
    "        image = image.to(DEVICE)    #mini-batch의 이미지 데이터 활용하기 위해 device에 할당\n",
    "        label = label.to(DEVICE)    #mini-batch의 레이블 데이터도 device에 할당\n",
    "        optimizer.zero_grad()       #기존의 device에 할당할 경우 과거에 이용한 데이터를 바탕으로 계산된 loss와 gradient값이 optimizer에 할당되어 있으므로 optimizer의 gradient값을 초기화\n",
    "        output = model(image)       #장비에 할당한 이미지 데이터를 MLP 모델의 input으로 output계산\n",
    "        loss = criterion(output, label) #계산된 output을 label값과 함께 기존에 정의한 cross entropy를 이용해 loss 계산\n",
    "        loss.backward()     #loss계산값을 바탕으로 back propagation -> 계산된 gradient 값 각 parameter에 할당\n",
    "        optimizer.step()    #각 파라미터에 할당된 graidient값을 이용해 파라미터 값 업데이트\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\t Train Loss: {:.6f}\".format(\n",
    "                Epoch, batch_idx * len(image),\n",
    "                len(train_loader.dataset), 100. *batch_idx / len(train_loader),\n",
    "                loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습되는 과정 중 검증 데이터에 대한 모델 성능을 확인\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()    #학습 과정, 학습 완료된 MLP 모델을 학습 상태가 아닌, 평가 상태로 지정\n",
    "    test_loss = 0   #test_loader 데이터를 이용해 loss 값 계산 위해 test_loss 0으로 임시설정\n",
    "    correct = 0     #학습 완료된 MLP 모델이 올바른 Class로 분류한 경우 세기 위한 변수\n",
    "\n",
    "    with torch.no_grad():   #torch.no_grad(): 모델 평가하는 단계에서 gradient통해 파라미터 값이 업데이트 되는 현상을 방지 \n",
    "        for image, label in test_loader:    #test_loader batch 단위로 저장 -> for 문으로 차례대로 접근\n",
    "            image = image.to(DEVICE)    #test_loader의 mini-batch내의 데이터 device에 할당\n",
    "            label = label.to(DEVICE)    #test_loader의 label 데이터 device에 할당\n",
    "            output = model(image)       #MLP 모델 output계산\n",
    "            test_loss += criterion(output, label).item()    #output과 label 값에 대해 CrossEntropy를 이용해 loss 계산-> test_loss 업데이트 \n",
    "            prediction = output.max(1, keepdim = True)[1]   #MLP 모델의 output -> 크기 10인 벡터값\n",
    "                                                            #계산된 벡터 값 내 가장 큰 값의 위치에 해당하는 클래스 예측\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()    #예측값 == label값일 때 correct 변수 1 증가(올바르게 예측한 횟수)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)   #현재까지의 test_loss를 batch 개수만큼 나누어 평균 loss 값 구함\n",
    "    test_accuracy = 100. *correct / len(test_loader.dataset)    #test_loader 데이터 중 얼마나 맞추었는지 정확도 계산\n",
    "    return test_loss, test_accuracy  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\t Train Loss: 2.432506\n",
      "Train Epoch: 1 [6400/60000(11%)]\t Train Loss: 2.180508\n",
      "Train Epoch: 1 [12800/60000(21%)]\t Train Loss: 2.382669\n",
      "Train Epoch: 1 [19200/60000(32%)]\t Train Loss: 2.309010\n",
      "Train Epoch: 1 [25600/60000(43%)]\t Train Loss: 2.299465\n",
      "Train Epoch: 1 [32000/60000(53%)]\t Train Loss: 2.314528\n",
      "Train Epoch: 1 [38400/60000(64%)]\t Train Loss: 2.227333\n",
      "Train Epoch: 1 [44800/60000(75%)]\t Train Loss: 2.302765\n",
      "Train Epoch: 1 [51200/60000(85%)]\t Train Loss: 2.290900\n",
      "Train Epoch: 1 [57600/60000(96%)]\t Train Loss: 2.337523\n",
      "\n",
      "[Epoch: 1], \t Test Loss: 0.0712, \t Test Accuracy: 15.93 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\t Train Loss: 2.325480\n",
      "Train Epoch: 2 [6400/60000(11%)]\t Train Loss: 2.306334\n",
      "Train Epoch: 2 [12800/60000(21%)]\t Train Loss: 2.221292\n",
      "Train Epoch: 2 [19200/60000(32%)]\t Train Loss: 2.292678\n",
      "Train Epoch: 2 [25600/60000(43%)]\t Train Loss: 2.248651\n",
      "Train Epoch: 2 [32000/60000(53%)]\t Train Loss: 2.244058\n",
      "Train Epoch: 2 [38400/60000(64%)]\t Train Loss: 2.234252\n",
      "Train Epoch: 2 [44800/60000(75%)]\t Train Loss: 2.208593\n",
      "Train Epoch: 2 [51200/60000(85%)]\t Train Loss: 1.951572\n",
      "Train Epoch: 2 [57600/60000(96%)]\t Train Loss: 2.027441\n",
      "\n",
      "[Epoch: 2], \t Test Loss: 0.0617, \t Test Accuracy: 40.55 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\t Train Loss: 2.070058\n",
      "Train Epoch: 3 [6400/60000(11%)]\t Train Loss: 1.910935\n",
      "Train Epoch: 3 [12800/60000(21%)]\t Train Loss: 1.732312\n",
      "Train Epoch: 3 [19200/60000(32%)]\t Train Loss: 1.917201\n",
      "Train Epoch: 3 [25600/60000(43%)]\t Train Loss: 1.527099\n",
      "Train Epoch: 3 [32000/60000(53%)]\t Train Loss: 1.556208\n",
      "Train Epoch: 3 [38400/60000(64%)]\t Train Loss: 1.317615\n",
      "Train Epoch: 3 [44800/60000(75%)]\t Train Loss: 1.532429\n",
      "Train Epoch: 3 [51200/60000(85%)]\t Train Loss: 1.291464\n",
      "Train Epoch: 3 [57600/60000(96%)]\t Train Loss: 1.314473\n",
      "\n",
      "[Epoch: 3], \t Test Loss: 0.0371, \t Test Accuracy: 62.49 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\t Train Loss: 1.140666\n",
      "Train Epoch: 4 [6400/60000(11%)]\t Train Loss: 1.143861\n",
      "Train Epoch: 4 [12800/60000(21%)]\t Train Loss: 1.388531\n",
      "Train Epoch: 4 [19200/60000(32%)]\t Train Loss: 1.660252\n",
      "Train Epoch: 4 [25600/60000(43%)]\t Train Loss: 1.068498\n",
      "Train Epoch: 4 [32000/60000(53%)]\t Train Loss: 0.964480\n",
      "Train Epoch: 4 [38400/60000(64%)]\t Train Loss: 0.921026\n",
      "Train Epoch: 4 [44800/60000(75%)]\t Train Loss: 0.751969\n",
      "Train Epoch: 4 [51200/60000(85%)]\t Train Loss: 1.097051\n",
      "Train Epoch: 4 [57600/60000(96%)]\t Train Loss: 0.959428\n",
      "\n",
      "[Epoch: 4], \t Test Loss: 0.0275, \t Test Accuracy: 71.42 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\t Train Loss: 1.153026\n",
      "Train Epoch: 5 [6400/60000(11%)]\t Train Loss: 0.920599\n",
      "Train Epoch: 5 [12800/60000(21%)]\t Train Loss: 0.976785\n",
      "Train Epoch: 5 [19200/60000(32%)]\t Train Loss: 1.067736\n",
      "Train Epoch: 5 [25600/60000(43%)]\t Train Loss: 0.762671\n",
      "Train Epoch: 5 [32000/60000(53%)]\t Train Loss: 0.995221\n",
      "Train Epoch: 5 [38400/60000(64%)]\t Train Loss: 0.873014\n",
      "Train Epoch: 5 [44800/60000(75%)]\t Train Loss: 1.250723\n",
      "Train Epoch: 5 [51200/60000(85%)]\t Train Loss: 1.085539\n",
      "Train Epoch: 5 [57600/60000(96%)]\t Train Loss: 0.919267\n",
      "\n",
      "[Epoch: 5], \t Test Loss: 0.0235, \t Test Accuracy: 76.41 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\t Train Loss: 1.143097\n",
      "Train Epoch: 6 [6400/60000(11%)]\t Train Loss: 0.845665\n",
      "Train Epoch: 6 [12800/60000(21%)]\t Train Loss: 1.369649\n",
      "Train Epoch: 6 [19200/60000(32%)]\t Train Loss: 0.699774\n",
      "Train Epoch: 6 [25600/60000(43%)]\t Train Loss: 1.107631\n",
      "Train Epoch: 6 [32000/60000(53%)]\t Train Loss: 0.747467\n",
      "Train Epoch: 6 [38400/60000(64%)]\t Train Loss: 0.770271\n",
      "Train Epoch: 6 [44800/60000(75%)]\t Train Loss: 0.823409\n",
      "Train Epoch: 6 [51200/60000(85%)]\t Train Loss: 1.232628\n",
      "Train Epoch: 6 [57600/60000(96%)]\t Train Loss: 1.407612\n",
      "\n",
      "[Epoch: 6], \t Test Loss: 0.0204, \t Test Accuracy: 80.32 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\t Train Loss: 0.740832\n",
      "Train Epoch: 7 [6400/60000(11%)]\t Train Loss: 0.800454\n",
      "Train Epoch: 7 [12800/60000(21%)]\t Train Loss: 0.946438\n",
      "Train Epoch: 7 [19200/60000(32%)]\t Train Loss: 0.984994\n",
      "Train Epoch: 7 [25600/60000(43%)]\t Train Loss: 0.610833\n",
      "Train Epoch: 7 [32000/60000(53%)]\t Train Loss: 0.725104\n",
      "Train Epoch: 7 [38400/60000(64%)]\t Train Loss: 0.770131\n",
      "Train Epoch: 7 [44800/60000(75%)]\t Train Loss: 0.761633\n",
      "Train Epoch: 7 [51200/60000(85%)]\t Train Loss: 0.787668\n",
      "Train Epoch: 7 [57600/60000(96%)]\t Train Loss: 0.730043\n",
      "\n",
      "[Epoch: 7], \t Test Loss: 0.0179, \t Test Accuracy: 83.04 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\t Train Loss: 0.778972\n",
      "Train Epoch: 8 [6400/60000(11%)]\t Train Loss: 0.409563\n",
      "Train Epoch: 8 [12800/60000(21%)]\t Train Loss: 0.579048\n",
      "Train Epoch: 8 [19200/60000(32%)]\t Train Loss: 0.411538\n",
      "Train Epoch: 8 [25600/60000(43%)]\t Train Loss: 0.702155\n",
      "Train Epoch: 8 [32000/60000(53%)]\t Train Loss: 1.203069\n",
      "Train Epoch: 8 [38400/60000(64%)]\t Train Loss: 0.475695\n",
      "Train Epoch: 8 [44800/60000(75%)]\t Train Loss: 0.433854\n",
      "Train Epoch: 8 [51200/60000(85%)]\t Train Loss: 0.535315\n",
      "Train Epoch: 8 [57600/60000(96%)]\t Train Loss: 0.629790\n",
      "\n",
      "[Epoch: 8], \t Test Loss: 0.0159, \t Test Accuracy: 85.06 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\t Train Loss: 0.598079\n",
      "Train Epoch: 9 [6400/60000(11%)]\t Train Loss: 0.846360\n",
      "Train Epoch: 9 [12800/60000(21%)]\t Train Loss: 0.226638\n",
      "Train Epoch: 9 [19200/60000(32%)]\t Train Loss: 0.637846\n",
      "Train Epoch: 9 [25600/60000(43%)]\t Train Loss: 0.947956\n",
      "Train Epoch: 9 [32000/60000(53%)]\t Train Loss: 0.496511\n",
      "Train Epoch: 9 [38400/60000(64%)]\t Train Loss: 0.504757\n",
      "Train Epoch: 9 [44800/60000(75%)]\t Train Loss: 0.861878\n",
      "Train Epoch: 9 [51200/60000(85%)]\t Train Loss: 0.721607\n",
      "Train Epoch: 9 [57600/60000(96%)]\t Train Loss: 0.887271\n",
      "\n",
      "[Epoch: 9], \t Test Loss: 0.0147, \t Test Accuracy: 85.84 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\t Train Loss: 0.578847\n",
      "Train Epoch: 10 [6400/60000(11%)]\t Train Loss: 0.615248\n",
      "Train Epoch: 10 [12800/60000(21%)]\t Train Loss: 0.821450\n",
      "Train Epoch: 10 [19200/60000(32%)]\t Train Loss: 0.615505\n",
      "Train Epoch: 10 [25600/60000(43%)]\t Train Loss: 0.653406\n",
      "Train Epoch: 10 [32000/60000(53%)]\t Train Loss: 0.364402\n",
      "Train Epoch: 10 [38400/60000(64%)]\t Train Loss: 0.462992\n",
      "Train Epoch: 10 [44800/60000(75%)]\t Train Loss: 0.336166\n",
      "Train Epoch: 10 [51200/60000(85%)]\t Train Loss: 0.378098\n",
      "Train Epoch: 10 [57600/60000(96%)]\t Train Loss: 0.471296\n",
      "\n",
      "[Epoch: 10], \t Test Loss: 0.0137, \t Test Accuracy: 86.85 %\n",
      "\n",
      "Train Epoch: 11 [0/60000(0%)]\t Train Loss: 0.430467\n",
      "Train Epoch: 11 [6400/60000(11%)]\t Train Loss: 0.526395\n",
      "Train Epoch: 11 [12800/60000(21%)]\t Train Loss: 0.545901\n",
      "Train Epoch: 11 [19200/60000(32%)]\t Train Loss: 0.782553\n",
      "Train Epoch: 11 [25600/60000(43%)]\t Train Loss: 1.000336\n",
      "Train Epoch: 11 [32000/60000(53%)]\t Train Loss: 0.431421\n",
      "Train Epoch: 11 [38400/60000(64%)]\t Train Loss: 0.513305\n",
      "Train Epoch: 11 [44800/60000(75%)]\t Train Loss: 0.408621\n",
      "Train Epoch: 11 [51200/60000(85%)]\t Train Loss: 0.526020\n",
      "Train Epoch: 11 [57600/60000(96%)]\t Train Loss: 0.602527\n",
      "\n",
      "[Epoch: 11], \t Test Loss: 0.0130, \t Test Accuracy: 87.53 %\n",
      "\n",
      "Train Epoch: 12 [0/60000(0%)]\t Train Loss: 0.536218\n",
      "Train Epoch: 12 [6400/60000(11%)]\t Train Loss: 0.202978\n",
      "Train Epoch: 12 [12800/60000(21%)]\t Train Loss: 0.398595\n",
      "Train Epoch: 12 [19200/60000(32%)]\t Train Loss: 0.460615\n",
      "Train Epoch: 12 [25600/60000(43%)]\t Train Loss: 0.809132\n",
      "Train Epoch: 12 [32000/60000(53%)]\t Train Loss: 0.282730\n",
      "Train Epoch: 12 [38400/60000(64%)]\t Train Loss: 0.621991\n",
      "Train Epoch: 12 [44800/60000(75%)]\t Train Loss: 0.315229\n",
      "Train Epoch: 12 [51200/60000(85%)]\t Train Loss: 0.259715\n",
      "Train Epoch: 12 [57600/60000(96%)]\t Train Loss: 0.922535\n",
      "\n",
      "[Epoch: 12], \t Test Loss: 0.0125, \t Test Accuracy: 87.98 %\n",
      "\n",
      "Train Epoch: 13 [0/60000(0%)]\t Train Loss: 0.443435\n",
      "Train Epoch: 13 [6400/60000(11%)]\t Train Loss: 0.501799\n",
      "Train Epoch: 13 [12800/60000(21%)]\t Train Loss: 0.934120\n",
      "Train Epoch: 13 [19200/60000(32%)]\t Train Loss: 0.551056\n",
      "Train Epoch: 13 [25600/60000(43%)]\t Train Loss: 0.633430\n",
      "Train Epoch: 13 [32000/60000(53%)]\t Train Loss: 0.670555\n",
      "Train Epoch: 13 [38400/60000(64%)]\t Train Loss: 0.775758\n",
      "Train Epoch: 13 [44800/60000(75%)]\t Train Loss: 0.465841\n",
      "Train Epoch: 13 [51200/60000(85%)]\t Train Loss: 0.357783\n",
      "Train Epoch: 13 [57600/60000(96%)]\t Train Loss: 0.677252\n",
      "\n",
      "[Epoch: 13], \t Test Loss: 0.0121, \t Test Accuracy: 88.48 %\n",
      "\n",
      "Train Epoch: 14 [0/60000(0%)]\t Train Loss: 0.448249\n",
      "Train Epoch: 14 [6400/60000(11%)]\t Train Loss: 0.799757\n",
      "Train Epoch: 14 [12800/60000(21%)]\t Train Loss: 0.478431\n",
      "Train Epoch: 14 [19200/60000(32%)]\t Train Loss: 0.605165\n",
      "Train Epoch: 14 [25600/60000(43%)]\t Train Loss: 0.433243\n",
      "Train Epoch: 14 [32000/60000(53%)]\t Train Loss: 0.365992\n",
      "Train Epoch: 14 [38400/60000(64%)]\t Train Loss: 0.660077\n",
      "Train Epoch: 14 [44800/60000(75%)]\t Train Loss: 0.368669\n",
      "Train Epoch: 14 [51200/60000(85%)]\t Train Loss: 0.388940\n",
      "Train Epoch: 14 [57600/60000(96%)]\t Train Loss: 0.448178\n",
      "\n",
      "[Epoch: 14], \t Test Loss: 0.0116, \t Test Accuracy: 88.88 %\n",
      "\n",
      "Train Epoch: 15 [0/60000(0%)]\t Train Loss: 0.378789\n",
      "Train Epoch: 15 [6400/60000(11%)]\t Train Loss: 0.792397\n",
      "Train Epoch: 15 [12800/60000(21%)]\t Train Loss: 0.308439\n",
      "Train Epoch: 15 [19200/60000(32%)]\t Train Loss: 0.507110\n",
      "Train Epoch: 15 [25600/60000(43%)]\t Train Loss: 0.278404\n",
      "Train Epoch: 15 [32000/60000(53%)]\t Train Loss: 0.606519\n",
      "Train Epoch: 15 [38400/60000(64%)]\t Train Loss: 0.595416\n",
      "Train Epoch: 15 [44800/60000(75%)]\t Train Loss: 0.113412\n",
      "Train Epoch: 15 [51200/60000(85%)]\t Train Loss: 0.369240\n",
      "Train Epoch: 15 [57600/60000(96%)]\t Train Loss: 0.513215\n",
      "\n",
      "[Epoch: 15], \t Test Loss: 0.0113, \t Test Accuracy: 89.17 %\n",
      "\n",
      "Train Epoch: 16 [0/60000(0%)]\t Train Loss: 0.323138\n",
      "Train Epoch: 16 [6400/60000(11%)]\t Train Loss: 0.244973\n",
      "Train Epoch: 16 [12800/60000(21%)]\t Train Loss: 0.367332\n",
      "Train Epoch: 16 [19200/60000(32%)]\t Train Loss: 0.443272\n",
      "Train Epoch: 16 [25600/60000(43%)]\t Train Loss: 0.386917\n",
      "Train Epoch: 16 [32000/60000(53%)]\t Train Loss: 0.458312\n",
      "Train Epoch: 16 [38400/60000(64%)]\t Train Loss: 0.426567\n",
      "Train Epoch: 16 [44800/60000(75%)]\t Train Loss: 0.394916\n",
      "Train Epoch: 16 [51200/60000(85%)]\t Train Loss: 0.418371\n",
      "Train Epoch: 16 [57600/60000(96%)]\t Train Loss: 0.704441\n",
      "\n",
      "[Epoch: 16], \t Test Loss: 0.0110, \t Test Accuracy: 89.42 %\n",
      "\n",
      "Train Epoch: 17 [0/60000(0%)]\t Train Loss: 0.510224\n",
      "Train Epoch: 17 [6400/60000(11%)]\t Train Loss: 0.481875\n",
      "Train Epoch: 17 [12800/60000(21%)]\t Train Loss: 0.514465\n",
      "Train Epoch: 17 [19200/60000(32%)]\t Train Loss: 0.382486\n",
      "Train Epoch: 17 [25600/60000(43%)]\t Train Loss: 0.813911\n",
      "Train Epoch: 17 [32000/60000(53%)]\t Train Loss: 0.170075\n",
      "Train Epoch: 17 [38400/60000(64%)]\t Train Loss: 0.283274\n",
      "Train Epoch: 17 [44800/60000(75%)]\t Train Loss: 0.409125\n",
      "Train Epoch: 17 [51200/60000(85%)]\t Train Loss: 0.312076\n",
      "Train Epoch: 17 [57600/60000(96%)]\t Train Loss: 0.207108\n",
      "\n",
      "[Epoch: 17], \t Test Loss: 0.0107, \t Test Accuracy: 89.77 %\n",
      "\n",
      "Train Epoch: 18 [0/60000(0%)]\t Train Loss: 0.562377\n",
      "Train Epoch: 18 [6400/60000(11%)]\t Train Loss: 0.432414\n",
      "Train Epoch: 18 [12800/60000(21%)]\t Train Loss: 0.657062\n",
      "Train Epoch: 18 [19200/60000(32%)]\t Train Loss: 0.585677\n",
      "Train Epoch: 18 [25600/60000(43%)]\t Train Loss: 0.362548\n",
      "Train Epoch: 18 [32000/60000(53%)]\t Train Loss: 0.313236\n",
      "Train Epoch: 18 [38400/60000(64%)]\t Train Loss: 0.604442\n",
      "Train Epoch: 18 [44800/60000(75%)]\t Train Loss: 0.452320\n",
      "Train Epoch: 18 [51200/60000(85%)]\t Train Loss: 0.270754\n",
      "Train Epoch: 18 [57600/60000(96%)]\t Train Loss: 0.402208\n",
      "\n",
      "[Epoch: 18], \t Test Loss: 0.0104, \t Test Accuracy: 90.04 %\n",
      "\n",
      "Train Epoch: 19 [0/60000(0%)]\t Train Loss: 0.437593\n",
      "Train Epoch: 19 [6400/60000(11%)]\t Train Loss: 0.718455\n",
      "Train Epoch: 19 [12800/60000(21%)]\t Train Loss: 0.318398\n",
      "Train Epoch: 19 [19200/60000(32%)]\t Train Loss: 0.393785\n",
      "Train Epoch: 19 [25600/60000(43%)]\t Train Loss: 0.470304\n",
      "Train Epoch: 19 [32000/60000(53%)]\t Train Loss: 0.533449\n",
      "Train Epoch: 19 [38400/60000(64%)]\t Train Loss: 0.231509\n",
      "Train Epoch: 19 [44800/60000(75%)]\t Train Loss: 0.634855\n",
      "Train Epoch: 19 [51200/60000(85%)]\t Train Loss: 0.456836\n",
      "Train Epoch: 19 [57600/60000(96%)]\t Train Loss: 0.592313\n",
      "\n",
      "[Epoch: 19], \t Test Loss: 0.0102, \t Test Accuracy: 90.22 %\n",
      "\n",
      "Train Epoch: 20 [0/60000(0%)]\t Train Loss: 0.563264\n",
      "Train Epoch: 20 [6400/60000(11%)]\t Train Loss: 0.326479\n",
      "Train Epoch: 20 [12800/60000(21%)]\t Train Loss: 0.466905\n",
      "Train Epoch: 20 [19200/60000(32%)]\t Train Loss: 0.489018\n",
      "Train Epoch: 20 [25600/60000(43%)]\t Train Loss: 0.680542\n",
      "Train Epoch: 20 [32000/60000(53%)]\t Train Loss: 0.454480\n",
      "Train Epoch: 20 [38400/60000(64%)]\t Train Loss: 0.527400\n",
      "Train Epoch: 20 [44800/60000(75%)]\t Train Loss: 0.359585\n",
      "Train Epoch: 20 [51200/60000(85%)]\t Train Loss: 0.555344\n",
      "Train Epoch: 20 [57600/60000(96%)]\t Train Loss: 0.246514\n",
      "\n",
      "[Epoch: 20], \t Test Loss: 0.0100, \t Test Accuracy: 90.41 %\n",
      "\n",
      "Train Epoch: 21 [0/60000(0%)]\t Train Loss: 0.218980\n",
      "Train Epoch: 21 [6400/60000(11%)]\t Train Loss: 0.319666\n",
      "Train Epoch: 21 [12800/60000(21%)]\t Train Loss: 0.378590\n",
      "Train Epoch: 21 [19200/60000(32%)]\t Train Loss: 0.766932\n",
      "Train Epoch: 21 [25600/60000(43%)]\t Train Loss: 0.771537\n",
      "Train Epoch: 21 [32000/60000(53%)]\t Train Loss: 0.772214\n",
      "Train Epoch: 21 [38400/60000(64%)]\t Train Loss: 0.513085\n",
      "Train Epoch: 21 [44800/60000(75%)]\t Train Loss: 0.347525\n",
      "Train Epoch: 21 [51200/60000(85%)]\t Train Loss: 0.305261\n",
      "Train Epoch: 21 [57600/60000(96%)]\t Train Loss: 0.246849\n",
      "\n",
      "[Epoch: 21], \t Test Loss: 0.0098, \t Test Accuracy: 90.68 %\n",
      "\n",
      "Train Epoch: 22 [0/60000(0%)]\t Train Loss: 0.282895\n",
      "Train Epoch: 22 [6400/60000(11%)]\t Train Loss: 0.336652\n",
      "Train Epoch: 22 [12800/60000(21%)]\t Train Loss: 0.304131\n",
      "Train Epoch: 22 [19200/60000(32%)]\t Train Loss: 0.670267\n",
      "Train Epoch: 22 [25600/60000(43%)]\t Train Loss: 0.149977\n",
      "Train Epoch: 22 [32000/60000(53%)]\t Train Loss: 0.738344\n",
      "Train Epoch: 22 [38400/60000(64%)]\t Train Loss: 0.579288\n",
      "Train Epoch: 22 [44800/60000(75%)]\t Train Loss: 0.430458\n",
      "Train Epoch: 22 [51200/60000(85%)]\t Train Loss: 0.452001\n",
      "Train Epoch: 22 [57600/60000(96%)]\t Train Loss: 0.353051\n",
      "\n",
      "[Epoch: 22], \t Test Loss: 0.0096, \t Test Accuracy: 90.94 %\n",
      "\n",
      "Train Epoch: 23 [0/60000(0%)]\t Train Loss: 0.266207\n",
      "Train Epoch: 23 [6400/60000(11%)]\t Train Loss: 0.416018\n",
      "Train Epoch: 23 [12800/60000(21%)]\t Train Loss: 0.666225\n",
      "Train Epoch: 23 [19200/60000(32%)]\t Train Loss: 0.487703\n",
      "Train Epoch: 23 [25600/60000(43%)]\t Train Loss: 0.369986\n",
      "Train Epoch: 23 [32000/60000(53%)]\t Train Loss: 0.418101\n",
      "Train Epoch: 23 [38400/60000(64%)]\t Train Loss: 0.557492\n",
      "Train Epoch: 23 [44800/60000(75%)]\t Train Loss: 0.512462\n",
      "Train Epoch: 23 [51200/60000(85%)]\t Train Loss: 0.204818\n",
      "Train Epoch: 23 [57600/60000(96%)]\t Train Loss: 0.303816\n",
      "\n",
      "[Epoch: 23], \t Test Loss: 0.0094, \t Test Accuracy: 91.00 %\n",
      "\n",
      "Train Epoch: 24 [0/60000(0%)]\t Train Loss: 0.292483\n",
      "Train Epoch: 24 [6400/60000(11%)]\t Train Loss: 0.385764\n",
      "Train Epoch: 24 [12800/60000(21%)]\t Train Loss: 0.514383\n",
      "Train Epoch: 24 [19200/60000(32%)]\t Train Loss: 0.697366\n",
      "Train Epoch: 24 [25600/60000(43%)]\t Train Loss: 0.310011\n",
      "Train Epoch: 24 [32000/60000(53%)]\t Train Loss: 0.350740\n",
      "Train Epoch: 24 [38400/60000(64%)]\t Train Loss: 0.563977\n",
      "Train Epoch: 24 [44800/60000(75%)]\t Train Loss: 0.194512\n",
      "Train Epoch: 24 [51200/60000(85%)]\t Train Loss: 0.199433\n",
      "Train Epoch: 24 [57600/60000(96%)]\t Train Loss: 0.578106\n",
      "\n",
      "[Epoch: 24], \t Test Loss: 0.0093, \t Test Accuracy: 90.96 %\n",
      "\n",
      "Train Epoch: 25 [0/60000(0%)]\t Train Loss: 0.563896\n",
      "Train Epoch: 25 [6400/60000(11%)]\t Train Loss: 0.213276\n",
      "Train Epoch: 25 [12800/60000(21%)]\t Train Loss: 0.512066\n",
      "Train Epoch: 25 [19200/60000(32%)]\t Train Loss: 0.463065\n",
      "Train Epoch: 25 [25600/60000(43%)]\t Train Loss: 0.480269\n",
      "Train Epoch: 25 [32000/60000(53%)]\t Train Loss: 0.180025\n",
      "Train Epoch: 25 [38400/60000(64%)]\t Train Loss: 0.328843\n",
      "Train Epoch: 25 [44800/60000(75%)]\t Train Loss: 0.127729\n",
      "Train Epoch: 25 [51200/60000(85%)]\t Train Loss: 0.355507\n",
      "Train Epoch: 25 [57600/60000(96%)]\t Train Loss: 0.751730\n",
      "\n",
      "[Epoch: 25], \t Test Loss: 0.0091, \t Test Accuracy: 91.20 %\n",
      "\n",
      "Train Epoch: 26 [0/60000(0%)]\t Train Loss: 0.297308\n",
      "Train Epoch: 26 [6400/60000(11%)]\t Train Loss: 0.375981\n",
      "Train Epoch: 26 [12800/60000(21%)]\t Train Loss: 0.154690\n",
      "Train Epoch: 26 [19200/60000(32%)]\t Train Loss: 0.229441\n",
      "Train Epoch: 26 [25600/60000(43%)]\t Train Loss: 0.592337\n",
      "Train Epoch: 26 [32000/60000(53%)]\t Train Loss: 0.192077\n",
      "Train Epoch: 26 [38400/60000(64%)]\t Train Loss: 0.275837\n",
      "Train Epoch: 26 [44800/60000(75%)]\t Train Loss: 0.157371\n",
      "Train Epoch: 26 [51200/60000(85%)]\t Train Loss: 0.232099\n",
      "Train Epoch: 26 [57600/60000(96%)]\t Train Loss: 0.306555\n",
      "\n",
      "[Epoch: 26], \t Test Loss: 0.0089, \t Test Accuracy: 91.45 %\n",
      "\n",
      "Train Epoch: 27 [0/60000(0%)]\t Train Loss: 0.330801\n",
      "Train Epoch: 27 [6400/60000(11%)]\t Train Loss: 0.349570\n",
      "Train Epoch: 27 [12800/60000(21%)]\t Train Loss: 0.513689\n",
      "Train Epoch: 27 [19200/60000(32%)]\t Train Loss: 0.257924\n",
      "Train Epoch: 27 [25600/60000(43%)]\t Train Loss: 0.181349\n",
      "Train Epoch: 27 [32000/60000(53%)]\t Train Loss: 0.387527\n",
      "Train Epoch: 27 [38400/60000(64%)]\t Train Loss: 0.524568\n",
      "Train Epoch: 27 [44800/60000(75%)]\t Train Loss: 0.301054\n",
      "Train Epoch: 27 [51200/60000(85%)]\t Train Loss: 0.913896\n",
      "Train Epoch: 27 [57600/60000(96%)]\t Train Loss: 0.515652\n",
      "\n",
      "[Epoch: 27], \t Test Loss: 0.0088, \t Test Accuracy: 91.56 %\n",
      "\n",
      "Train Epoch: 28 [0/60000(0%)]\t Train Loss: 0.212849\n",
      "Train Epoch: 28 [6400/60000(11%)]\t Train Loss: 0.475957\n",
      "Train Epoch: 28 [12800/60000(21%)]\t Train Loss: 0.510911\n",
      "Train Epoch: 28 [19200/60000(32%)]\t Train Loss: 0.387894\n",
      "Train Epoch: 28 [25600/60000(43%)]\t Train Loss: 0.383042\n",
      "Train Epoch: 28 [32000/60000(53%)]\t Train Loss: 0.441215\n",
      "Train Epoch: 28 [38400/60000(64%)]\t Train Loss: 0.339201\n",
      "Train Epoch: 28 [44800/60000(75%)]\t Train Loss: 0.084107\n",
      "Train Epoch: 28 [51200/60000(85%)]\t Train Loss: 0.509247\n",
      "Train Epoch: 28 [57600/60000(96%)]\t Train Loss: 0.184439\n",
      "\n",
      "[Epoch: 28], \t Test Loss: 0.0086, \t Test Accuracy: 91.62 %\n",
      "\n",
      "Train Epoch: 29 [0/60000(0%)]\t Train Loss: 0.351250\n",
      "Train Epoch: 29 [6400/60000(11%)]\t Train Loss: 0.626377\n",
      "Train Epoch: 29 [12800/60000(21%)]\t Train Loss: 0.552794\n",
      "Train Epoch: 29 [19200/60000(32%)]\t Train Loss: 0.185494\n",
      "Train Epoch: 29 [25600/60000(43%)]\t Train Loss: 0.863831\n",
      "Train Epoch: 29 [32000/60000(53%)]\t Train Loss: 0.261596\n",
      "Train Epoch: 29 [38400/60000(64%)]\t Train Loss: 0.499538\n",
      "Train Epoch: 29 [44800/60000(75%)]\t Train Loss: 0.668901\n",
      "Train Epoch: 29 [51200/60000(85%)]\t Train Loss: 0.275072\n",
      "Train Epoch: 29 [57600/60000(96%)]\t Train Loss: 0.428209\n",
      "\n",
      "[Epoch: 29], \t Test Loss: 0.0085, \t Test Accuracy: 91.74 %\n",
      "\n",
      "Train Epoch: 30 [0/60000(0%)]\t Train Loss: 0.603583\n",
      "Train Epoch: 30 [6400/60000(11%)]\t Train Loss: 0.412279\n",
      "Train Epoch: 30 [12800/60000(21%)]\t Train Loss: 0.569965\n",
      "Train Epoch: 30 [19200/60000(32%)]\t Train Loss: 0.326903\n",
      "Train Epoch: 30 [25600/60000(43%)]\t Train Loss: 0.258355\n",
      "Train Epoch: 30 [32000/60000(53%)]\t Train Loss: 0.075626\n",
      "Train Epoch: 30 [38400/60000(64%)]\t Train Loss: 0.434332\n",
      "Train Epoch: 30 [44800/60000(75%)]\t Train Loss: 0.647496\n",
      "Train Epoch: 30 [51200/60000(85%)]\t Train Loss: 0.289334\n",
      "Train Epoch: 30 [57600/60000(96%)]\t Train Loss: 0.405670\n",
      "\n",
      "[Epoch: 30], \t Test Loss: 0.0084, \t Test Accuracy: 91.95 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## MLP 학습 -> Train, Test set의 Loss 및 Test Set Accuracy 확인\n",
    "\n",
    "for Epoch in range (1, EPOCHS +1 ):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   #train_loaer 학습데이터로 train model 실행, log_interval: 학습 과정 출력\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader) #각 epoch별로 출력되는 loss 값과 accuracy 계산\n",
    "    print(\"\\n[Epoch: {}], \\t Test Loss: {:.4f}, \\t Test Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}